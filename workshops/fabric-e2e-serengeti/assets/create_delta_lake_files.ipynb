{"cells":[{"cell_type":"code","source":["import os"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"5d1aa8d0-49b9-4260-8dac-2ba686428313","statement_id":6,"state":"finished","livy_statement_state":"available","queued_time":"2023-04-20T06:44:08.5492385Z","session_start_time":"2023-04-20T06:44:09.2362833Z","execution_start_time":"2023-04-20T06:44:09.4082676Z","execution_finish_time":"2023-04-20T06:44:09.8363963Z","spark_jobs":{"numbers":{"RUNNING":0,"SUCCEEDED":0,"FAILED":0,"UNKNOWN":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"901d891f-e229-43f3-ba8e-a558959907db"},"text/plain":"StatementMeta(, 5d1aa8d0-49b9-4260-8dac-2ba686428313, 6, Finished, Available)"},"metadata":{}}],"execution_count":3,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"7960e481-fe2a-463f-8118-7cad08f2c191"},{"cell_type":"code","source":["# Annotations files S01 to S10\n","annotations_file_names = ['Annotations-S01.parquet', 'Annotations-S02.parquet', 'Annotations-S03.parquet', 'Annotations-S04.parquet', 'Annotations-S05.parquet', 'Annotations-S06.parquet', 'Annotations-S07.parquet', 'Annotations-S08.parquet', 'Annotations-S09.parquet', 'Annotations-S10.parquet']\n","\n","# Images files S01 to S10\n","images_file_names = ['Images-S01.parquet', 'Images-S02.parquet', 'Images-S03.parquet', 'Images-S04.parquet', 'Images-S05.parquet', 'Images-S06.parquet', 'Images-S07.parquet', 'Images-S08.parquet', 'Images-S09.parquet', 'Images-S10.parquet']\n","\n","# S11 files\n","s11_annotations_file_name = 'Annotations-S11.parquet'\n","s11_images_file_name = 'Images-S11.parquet'\n","\n","# Categories file\n","categories_file = 'Categories.parquet'\n","\n","# file paths\n","input_dir = 'Files/metadata/parquet'\n","output_dir = 'Files/metadata/delta-lake'"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"5d1aa8d0-49b9-4260-8dac-2ba686428313","statement_id":7,"state":"finished","livy_statement_state":"available","queued_time":"2023-04-20T06:44:08.5546358Z","session_start_time":null,"execution_start_time":"2023-04-20T06:44:10.2004307Z","execution_finish_time":"2023-04-20T06:44:10.5564444Z","spark_jobs":{"numbers":{"RUNNING":0,"SUCCEEDED":0,"FAILED":0,"UNKNOWN":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"a1c0a454-5d4f-4743-a0bb-e80f5cb8290b"},"text/plain":"StatementMeta(, 5d1aa8d0-49b9-4260-8dac-2ba686428313, 7, Finished, Available)"},"metadata":{}}],"execution_count":4,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"c71bc4ba-ae14-4f00-882d-13aba7555452"},{"cell_type":"code","source":["# read the annotations parquet files using spark and append as one dataframe\n","annotations_df = spark.read.parquet(os.path.join(input_dir, annotations_file_names[0]))\n","for file_name in annotations_file_names[1:]:\n","    annotations_df = annotations_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\n","print(f'annotations : {annotations_df.count()}')\n","\n","# read the images parquet files using spark and append as one dataframe\n","images_df = spark.read.parquet(os.path.join(input_dir, images_file_names[0]))\n","for file_name in images_file_names[1:]:\n","    images_df = images_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\n","print(f'images : {images_df.count()}')\n","\n","# read the categories parquet file using spark\n","categories_df = spark.read.parquet(os.path.join(input_dir, categories_file))\n","print(f'categories : {categories_df.count()}')"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"5d1aa8d0-49b9-4260-8dac-2ba686428313","statement_id":10,"state":"finished","livy_statement_state":"available","queued_time":"2023-04-20T06:46:19.958477Z","session_start_time":null,"execution_start_time":"2023-04-20T06:46:20.2847115Z","execution_finish_time":"2023-04-20T06:46:53.1462521Z","spark_jobs":{"numbers":{"RUNNING":0,"SUCCEEDED":27,"FAILED":0,"UNKNOWN":0},"jobs":[{"dataWritten":0,"dataRead":59,"rowCount":1,"jobId":65,"name":"count at NativeMethodAccessorImpl.java:0","description":"Job group for statement 10:\n# read the annotations parquet files using spark and append as one dataframe\nannotations_df = spark.read.parquet(os.path.join(input_dir, annotations_file_names[0]))\nfor file_name in annotations_file_names[1:]:\n    annotations_df = annotations_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\nprint(f'annotations : {annotations_df.count()}')\n\n# read the images parquet files using spark and append as one dataframe\nimages_df = spark.read.parquet(os.path.join(input_dir, images_file_names[0]))\nfor file_name in images_file_names[1:]:\n    images_df = images_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\nprint(f'images : {images_df.count()}')\n\n# read the categories parquet file using spark\ncategories_df = spark.read.parquet(os.path.join(input_dir, categories_file))\nprint(f'categories : {categories_df.count()}')","submissionTime":"2023-04-20T06:46:51.204GMT","completionTime":"2023-04-20T06:46:51.231GMT","stageIds":[78,79],"jobGroup":"10","status":"SUCCEEDED","numTasks":2,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":59,"dataRead":572,"rowCount":62,"jobId":64,"name":"count at NativeMethodAccessorImpl.java:0","description":"Job group for statement 10:\n# read the annotations parquet files using spark and append as one dataframe\nannotations_df = spark.read.parquet(os.path.join(input_dir, annotations_file_names[0]))\nfor file_name in annotations_file_names[1:]:\n    annotations_df = annotations_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\nprint(f'annotations : {annotations_df.count()}')\n\n# read the images parquet files using spark and append as one dataframe\nimages_df = spark.read.parquet(os.path.join(input_dir, images_file_names[0]))\nfor file_name in images_file_names[1:]:\n    images_df = images_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\nprint(f'images : {images_df.count()}')\n\n# read the categories parquet file using spark\ncategories_df = spark.read.parquet(os.path.join(input_dir, categories_file))\nprint(f'categories : {categories_df.count()}')","submissionTime":"2023-04-20T06:46:50.142GMT","completionTime":"2023-04-20T06:46:51.194GMT","stageIds":[77],"jobGroup":"10","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":63,"name":"parquet at <unknown>:0","description":"Job group for statement 10:\n# read the annotations parquet files using spark and append as one dataframe\nannotations_df = spark.read.parquet(os.path.join(input_dir, annotations_file_names[0]))\nfor file_name in annotations_file_names[1:]:\n    annotations_df = annotations_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\nprint(f'annotations : {annotations_df.count()}')\n\n# read the images parquet files using spark and append as one dataframe\nimages_df = spark.read.parquet(os.path.join(input_dir, images_file_names[0]))\nfor file_name in images_file_names[1:]:\n    images_df = images_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\nprint(f'images : {images_df.count()}')\n\n# read the categories parquet file using spark\ncategories_df = spark.read.parquet(os.path.join(input_dir, categories_file))\nprint(f'categories : {categories_df.count()}')","submissionTime":"2023-04-20T06:46:49.604GMT","completionTime":"2023-04-20T06:46:50.048GMT","stageIds":[76],"jobGroup":"10","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":2662,"rowCount":47,"jobId":62,"name":"count at NativeMethodAccessorImpl.java:0","description":"Job group for statement 10:\n# read the annotations parquet files using spark and append as one dataframe\nannotations_df = spark.read.parquet(os.path.join(input_dir, annotations_file_names[0]))\nfor file_name in annotations_file_names[1:]:\n    annotations_df = annotations_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\nprint(f'annotations : {annotations_df.count()}')\n\n# read the images parquet files using spark and append as one dataframe\nimages_df = spark.read.parquet(os.path.join(input_dir, images_file_names[0]))\nfor file_name in images_file_names[1:]:\n    images_df = images_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\nprint(f'images : {images_df.count()}')\n\n# read the categories parquet file using spark\ncategories_df = spark.read.parquet(os.path.join(input_dir, categories_file))\nprint(f'categories : {categories_df.count()}')","submissionTime":"2023-04-20T06:46:49.040GMT","completionTime":"2023-04-20T06:46:49.067GMT","stageIds":[74,75],"jobGroup":"10","status":"SUCCEEDED","numTasks":48,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":47,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":2662,"dataRead":68315,"rowCount":6679086,"jobId":61,"name":"count at NativeMethodAccessorImpl.java:0","description":"Job group for statement 10:\n# read the annotations parquet files using spark and append as one dataframe\nannotations_df = spark.read.parquet(os.path.join(input_dir, annotations_file_names[0]))\nfor file_name in annotations_file_names[1:]:\n    annotations_df = annotations_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\nprint(f'annotations : {annotations_df.count()}')\n\n# read the images parquet files using spark and append as one dataframe\nimages_df = spark.read.parquet(os.path.join(input_dir, images_file_names[0]))\nfor file_name in images_file_names[1:]:\n    images_df = images_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\nprint(f'images : {images_df.count()}')\n\n# read the categories parquet file using spark\ncategories_df = spark.read.parquet(os.path.join(input_dir, categories_file))\nprint(f'categories : {categories_df.count()}')","submissionTime":"2023-04-20T06:46:45.582GMT","completionTime":"2023-04-20T06:46:49.026GMT","stageIds":[73],"jobGroup":"10","status":"SUCCEEDED","numTasks":47,"numActiveTasks":0,"numCompletedTasks":47,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":47,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":60,"name":"parquet at <unknown>:0","description":"Job group for statement 10:\n# read the annotations parquet files using spark and append as one dataframe\nannotations_df = spark.read.parquet(os.path.join(input_dir, annotations_file_names[0]))\nfor file_name in annotations_file_names[1:]:\n    annotations_df = annotations_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\nprint(f'annotations : {annotations_df.count()}')\n\n# read the images parquet files using spark and append as one dataframe\nimages_df = spark.read.parquet(os.path.join(input_dir, images_file_names[0]))\nfor file_name in images_file_names[1:]:\n    images_df = images_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\nprint(f'images : {images_df.count()}')\n\n# read the categories parquet file using spark\ncategories_df = spark.read.parquet(os.path.join(input_dir, categories_file))\nprint(f'categories : {categories_df.count()}')","submissionTime":"2023-04-20T06:46:44.825GMT","completionTime":"2023-04-20T06:46:45.301GMT","stageIds":[72],"jobGroup":"10","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":59,"name":"parquet at <unknown>:0","description":"Job group for statement 10:\n# read the annotations parquet files using spark and append as one dataframe\nannotations_df = spark.read.parquet(os.path.join(input_dir, annotations_file_names[0]))\nfor file_name in annotations_file_names[1:]:\n    annotations_df = annotations_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\nprint(f'annotations : {annotations_df.count()}')\n\n# read the images parquet files using spark and append as one dataframe\nimages_df = spark.read.parquet(os.path.join(input_dir, images_file_names[0]))\nfor file_name in images_file_names[1:]:\n    images_df = images_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\nprint(f'images : {images_df.count()}')\n\n# read the categories parquet file using spark\ncategories_df = spark.read.parquet(os.path.join(input_dir, categories_file))\nprint(f'categories : {categories_df.count()}')","submissionTime":"2023-04-20T06:46:43.894GMT","completionTime":"2023-04-20T06:46:44.347GMT","stageIds":[71],"jobGroup":"10","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":58,"name":"parquet at <unknown>:0","description":"Job group for statement 10:\n# read the annotations parquet files using spark and append as one dataframe\nannotations_df = spark.read.parquet(os.path.join(input_dir, annotations_file_names[0]))\nfor file_name in annotations_file_names[1:]:\n    annotations_df = annotations_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\nprint(f'annotations : {annotations_df.count()}')\n\n# read the images parquet files using spark and append as one dataframe\nimages_df = spark.read.parquet(os.path.join(input_dir, images_file_names[0]))\nfor file_name in images_file_names[1:]:\n    images_df = images_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\nprint(f'images : {images_df.count()}')\n\n# read the categories parquet file using spark\ncategories_df = spark.read.parquet(os.path.join(input_dir, categories_file))\nprint(f'categories : {categories_df.count()}')","submissionTime":"2023-04-20T06:46:42.984GMT","completionTime":"2023-04-20T06:46:43.446GMT","stageIds":[70],"jobGroup":"10","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":57,"name":"parquet at <unknown>:0","description":"Job group for statement 10:\n# read the annotations parquet files using spark and append as one dataframe\nannotations_df = spark.read.parquet(os.path.join(input_dir, annotations_file_names[0]))\nfor file_name in annotations_file_names[1:]:\n    annotations_df = annotations_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\nprint(f'annotations : {annotations_df.count()}')\n\n# read the images parquet files using spark and append as one dataframe\nimages_df = spark.read.parquet(os.path.join(input_dir, images_file_names[0]))\nfor file_name in images_file_names[1:]:\n    images_df = images_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\nprint(f'images : {images_df.count()}')\n\n# read the categories parquet file using spark\ncategories_df = spark.read.parquet(os.path.join(input_dir, categories_file))\nprint(f'categories : {categories_df.count()}')","submissionTime":"2023-04-20T06:46:42.030GMT","completionTime":"2023-04-20T06:46:42.460GMT","stageIds":[69],"jobGroup":"10","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":56,"name":"parquet at <unknown>:0","description":"Job group for statement 10:\n# read the annotations parquet files using spark and append as one dataframe\nannotations_df = spark.read.parquet(os.path.join(input_dir, annotations_file_names[0]))\nfor file_name in annotations_file_names[1:]:\n    annotations_df = annotations_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\nprint(f'annotations : {annotations_df.count()}')\n\n# read the images parquet files using spark and append as one dataframe\nimages_df = spark.read.parquet(os.path.join(input_dir, images_file_names[0]))\nfor file_name in images_file_names[1:]:\n    images_df = images_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\nprint(f'images : {images_df.count()}')\n\n# read the categories parquet file using spark\ncategories_df = spark.read.parquet(os.path.join(input_dir, categories_file))\nprint(f'categories : {categories_df.count()}')","submissionTime":"2023-04-20T06:46:41.088GMT","completionTime":"2023-04-20T06:46:41.524GMT","stageIds":[68],"jobGroup":"10","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":55,"name":"parquet at <unknown>:0","description":"Job group for statement 10:\n# read the annotations parquet files using spark and append as one dataframe\nannotations_df = spark.read.parquet(os.path.join(input_dir, annotations_file_names[0]))\nfor file_name in annotations_file_names[1:]:\n    annotations_df = annotations_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\nprint(f'annotations : {annotations_df.count()}')\n\n# read the images parquet files using spark and append as one dataframe\nimages_df = spark.read.parquet(os.path.join(input_dir, images_file_names[0]))\nfor file_name in images_file_names[1:]:\n    images_df = images_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\nprint(f'images : {images_df.count()}')\n\n# read the categories parquet file using spark\ncategories_df = spark.read.parquet(os.path.join(input_dir, categories_file))\nprint(f'categories : {categories_df.count()}')","submissionTime":"2023-04-20T06:46:40.200GMT","completionTime":"2023-04-20T06:46:40.621GMT","stageIds":[67],"jobGroup":"10","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":54,"name":"parquet at <unknown>:0","description":"Job group for statement 10:\n# read the annotations parquet files using spark and append as one dataframe\nannotations_df = spark.read.parquet(os.path.join(input_dir, annotations_file_names[0]))\nfor file_name in annotations_file_names[1:]:\n    annotations_df = annotations_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\nprint(f'annotations : {annotations_df.count()}')\n\n# read the images parquet files using spark and append as one dataframe\nimages_df = spark.read.parquet(os.path.join(input_dir, images_file_names[0]))\nfor file_name in images_file_names[1:]:\n    images_df = images_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\nprint(f'images : {images_df.count()}')\n\n# read the categories parquet file using spark\ncategories_df = spark.read.parquet(os.path.join(input_dir, categories_file))\nprint(f'categories : {categories_df.count()}')","submissionTime":"2023-04-20T06:46:39.177GMT","completionTime":"2023-04-20T06:46:39.739GMT","stageIds":[66],"jobGroup":"10","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":53,"name":"parquet at <unknown>:0","description":"Job group for statement 10:\n# read the annotations parquet files using spark and append as one dataframe\nannotations_df = spark.read.parquet(os.path.join(input_dir, annotations_file_names[0]))\nfor file_name in annotations_file_names[1:]:\n    annotations_df = annotations_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\nprint(f'annotations : {annotations_df.count()}')\n\n# read the images parquet files using spark and append as one dataframe\nimages_df = spark.read.parquet(os.path.join(input_dir, images_file_names[0]))\nfor file_name in images_file_names[1:]:\n    images_df = images_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\nprint(f'images : {images_df.count()}')\n\n# read the categories parquet file using spark\ncategories_df = spark.read.parquet(os.path.join(input_dir, categories_file))\nprint(f'categories : {categories_df.count()}')","submissionTime":"2023-04-20T06:46:38.236GMT","completionTime":"2023-04-20T06:46:38.685GMT","stageIds":[65],"jobGroup":"10","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":52,"name":"parquet at <unknown>:0","description":"Job group for statement 10:\n# read the annotations parquet files using spark and append as one dataframe\nannotations_df = spark.read.parquet(os.path.join(input_dir, annotations_file_names[0]))\nfor file_name in annotations_file_names[1:]:\n    annotations_df = annotations_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\nprint(f'annotations : {annotations_df.count()}')\n\n# read the images parquet files using spark and append as one dataframe\nimages_df = spark.read.parquet(os.path.join(input_dir, images_file_names[0]))\nfor file_name in images_file_names[1:]:\n    images_df = images_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\nprint(f'images : {images_df.count()}')\n\n# read the categories parquet file using spark\ncategories_df = spark.read.parquet(os.path.join(input_dir, categories_file))\nprint(f'categories : {categories_df.count()}')","submissionTime":"2023-04-20T06:46:37.260GMT","completionTime":"2023-04-20T06:46:37.713GMT","stageIds":[64],"jobGroup":"10","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":51,"name":"parquet at <unknown>:0","description":"Job group for statement 10:\n# read the annotations parquet files using spark and append as one dataframe\nannotations_df = spark.read.parquet(os.path.join(input_dir, annotations_file_names[0]))\nfor file_name in annotations_file_names[1:]:\n    annotations_df = annotations_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\nprint(f'annotations : {annotations_df.count()}')\n\n# read the images parquet files using spark and append as one dataframe\nimages_df = spark.read.parquet(os.path.join(input_dir, images_file_names[0]))\nfor file_name in images_file_names[1:]:\n    images_df = images_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\nprint(f'images : {images_df.count()}')\n\n# read the categories parquet file using spark\ncategories_df = spark.read.parquet(os.path.join(input_dir, categories_file))\nprint(f'categories : {categories_df.count()}')","submissionTime":"2023-04-20T06:46:36.306GMT","completionTime":"2023-04-20T06:46:36.776GMT","stageIds":[63],"jobGroup":"10","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":2774,"rowCount":49,"jobId":50,"name":"count at NativeMethodAccessorImpl.java:0","description":"Job group for statement 10:\n# read the annotations parquet files using spark and append as one dataframe\nannotations_df = spark.read.parquet(os.path.join(input_dir, annotations_file_names[0]))\nfor file_name in annotations_file_names[1:]:\n    annotations_df = annotations_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\nprint(f'annotations : {annotations_df.count()}')\n\n# read the images parquet files using spark and append as one dataframe\nimages_df = spark.read.parquet(os.path.join(input_dir, images_file_names[0]))\nfor file_name in images_file_names[1:]:\n    images_df = images_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\nprint(f'images : {images_df.count()}')\n\n# read the categories parquet file using spark\ncategories_df = spark.read.parquet(os.path.join(input_dir, categories_file))\nprint(f'categories : {categories_df.count()}')","submissionTime":"2023-04-20T06:46:35.793GMT","completionTime":"2023-04-20T06:46:35.829GMT","stageIds":[61,62],"jobGroup":"10","status":"SUCCEEDED","numTasks":50,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":49,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":2774,"dataRead":91281,"rowCount":6755139,"jobId":49,"name":"count at NativeMethodAccessorImpl.java:0","description":"Job group for statement 10:\n# read the annotations parquet files using spark and append as one dataframe\nannotations_df = spark.read.parquet(os.path.join(input_dir, annotations_file_names[0]))\nfor file_name in annotations_file_names[1:]:\n    annotations_df = annotations_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\nprint(f'annotations : {annotations_df.count()}')\n\n# read the images parquet files using spark and append as one dataframe\nimages_df = spark.read.parquet(os.path.join(input_dir, images_file_names[0]))\nfor file_name in images_file_names[1:]:\n    images_df = images_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\nprint(f'images : {images_df.count()}')\n\n# read the categories parquet file using spark\ncategories_df = spark.read.parquet(os.path.join(input_dir, categories_file))\nprint(f'categories : {categories_df.count()}')","submissionTime":"2023-04-20T06:46:32.370GMT","completionTime":"2023-04-20T06:46:35.780GMT","stageIds":[60],"jobGroup":"10","status":"SUCCEEDED","numTasks":49,"numActiveTasks":0,"numCompletedTasks":49,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":49,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":48,"name":"parquet at <unknown>:0","description":"Job group for statement 10:\n# read the annotations parquet files using spark and append as one dataframe\nannotations_df = spark.read.parquet(os.path.join(input_dir, annotations_file_names[0]))\nfor file_name in annotations_file_names[1:]:\n    annotations_df = annotations_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\nprint(f'annotations : {annotations_df.count()}')\n\n# read the images parquet files using spark and append as one dataframe\nimages_df = spark.read.parquet(os.path.join(input_dir, images_file_names[0]))\nfor file_name in images_file_names[1:]:\n    images_df = images_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\nprint(f'images : {images_df.count()}')\n\n# read the categories parquet file using spark\ncategories_df = spark.read.parquet(os.path.join(input_dir, categories_file))\nprint(f'categories : {categories_df.count()}')","submissionTime":"2023-04-20T06:46:31.617GMT","completionTime":"2023-04-20T06:46:32.087GMT","stageIds":[59],"jobGroup":"10","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":47,"name":"parquet at <unknown>:0","description":"Job group for statement 10:\n# read the annotations parquet files using spark and append as one dataframe\nannotations_df = spark.read.parquet(os.path.join(input_dir, annotations_file_names[0]))\nfor file_name in annotations_file_names[1:]:\n    annotations_df = annotations_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\nprint(f'annotations : {annotations_df.count()}')\n\n# read the images parquet files using spark and append as one dataframe\nimages_df = spark.read.parquet(os.path.join(input_dir, images_file_names[0]))\nfor file_name in images_file_names[1:]:\n    images_df = images_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\nprint(f'images : {images_df.count()}')\n\n# read the categories parquet file using spark\ncategories_df = spark.read.parquet(os.path.join(input_dir, categories_file))\nprint(f'categories : {categories_df.count()}')","submissionTime":"2023-04-20T06:46:30.732GMT","completionTime":"2023-04-20T06:46:31.156GMT","stageIds":[58],"jobGroup":"10","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":46,"name":"parquet at <unknown>:0","description":"Job group for statement 10:\n# read the annotations parquet files using spark and append as one dataframe\nannotations_df = spark.read.parquet(os.path.join(input_dir, annotations_file_names[0]))\nfor file_name in annotations_file_names[1:]:\n    annotations_df = annotations_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\nprint(f'annotations : {annotations_df.count()}')\n\n# read the images parquet files using spark and append as one dataframe\nimages_df = spark.read.parquet(os.path.join(input_dir, images_file_names[0]))\nfor file_name in images_file_names[1:]:\n    images_df = images_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\nprint(f'images : {images_df.count()}')\n\n# read the categories parquet file using spark\ncategories_df = spark.read.parquet(os.path.join(input_dir, categories_file))\nprint(f'categories : {categories_df.count()}')","submissionTime":"2023-04-20T06:46:29.828GMT","completionTime":"2023-04-20T06:46:30.264GMT","stageIds":[57],"jobGroup":"10","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"e856f324-d414-4db2-8083-972fd288f1b9"},"text/plain":"StatementMeta(, 5d1aa8d0-49b9-4260-8dac-2ba686428313, 10, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["annotations : 6755090\nimages : 6679039\ncategories : 61\n"]}],"execution_count":7,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"c5f6a001-4ed6-4fdb-abf0-cc71df93ff4b"},{"cell_type":"code","source":["# write the annotations dataframe to a delta lake table\n","annotations_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'annotations'))\n","\n","# write the images dataframe to a delta lake table\n","images_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'images'))\n","\n","# write the categories dataframe to a delta lake table\n","categories_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'categories'))"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"5d1aa8d0-49b9-4260-8dac-2ba686428313","statement_id":13,"state":"finished","livy_statement_state":"available","queued_time":"2023-04-20T06:48:55.7347141Z","session_start_time":null,"execution_start_time":"2023-04-20T06:48:56.0265366Z","execution_finish_time":"2023-04-20T06:49:33.5361004Z","spark_jobs":{"numbers":{"RUNNING":0,"SUCCEEDED":15,"FAILED":0,"UNKNOWN":0},"jobs":[{"dataWritten":0,"dataRead":4322,"rowCount":50,"jobId":116,"name":"$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:95","description":"Delta: Job group for statement 13:\n# write the annotations dataframe to a delta lake table\nannotations_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'annotations'))\n\n# write the images dataframe to a delta lake table\nimages_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'images'))\n\n# write the categories dataframe to a delta lake table\ncategories_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'categories')): Compute snapshot for version: 2","submissionTime":"2023-04-20T06:49:31.619GMT","completionTime":"2023-04-20T06:49:31.649GMT","stageIds":[171,169,170],"jobGroup":"13","status":"SUCCEEDED","numTasks":54,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":53,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":4322,"dataRead":4639,"rowCount":60,"jobId":115,"name":"$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:95","description":"Delta: Job group for statement 13:\n# write the annotations dataframe to a delta lake table\nannotations_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'annotations'))\n\n# write the images dataframe to a delta lake table\nimages_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'images'))\n\n# write the categories dataframe to a delta lake table\ncategories_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'categories')): Compute snapshot for version: 2","submissionTime":"2023-04-20T06:49:31.417GMT","completionTime":"2023-04-20T06:49:31.600GMT","stageIds":[168,167],"jobGroup":"13","status":"SUCCEEDED","numTasks":53,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":3,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":4639,"dataRead":3179,"rowCount":20,"jobId":114,"name":"$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:95","description":"Delta: Job group for statement 13:\n# write the annotations dataframe to a delta lake table\nannotations_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'annotations'))\n\n# write the images dataframe to a delta lake table\nimages_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'images'))\n\n# write the categories dataframe to a delta lake table\ncategories_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'categories')): Compute snapshot for version: 2","submissionTime":"2023-04-20T06:49:30.935GMT","completionTime":"2023-04-20T06:49:31.293GMT","stageIds":[166],"jobGroup":"13","status":"SUCCEEDED","numTasks":3,"numActiveTasks":0,"numCompletedTasks":3,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":3,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":1879,"rowCount":4,"jobId":113,"name":"$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:95","description":"Job group for statement 13:\n# write the annotations dataframe to a delta lake table\nannotations_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'annotations'))\n\n# write the images dataframe to a delta lake table\nimages_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'images'))\n\n# write the categories dataframe to a delta lake table\ncategories_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'categories'))","submissionTime":"2023-04-20T06:49:29.319GMT","completionTime":"2023-04-20T06:49:29.367GMT","stageIds":[165,164],"jobGroup":"13","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":2,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":2163,"dataRead":2333,"rowCount":122,"jobId":112,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 13:\n# write the annotations dataframe to a delta lake table\nannotations_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'annotations'))\n\n# write the images dataframe to a delta lake table\nimages_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'images'))\n\n# write the categories dataframe to a delta lake table\ncategories_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'categories'))","submissionTime":"2023-04-20T06:49:26.420GMT","completionTime":"2023-04-20T06:49:29.253GMT","stageIds":[163],"jobGroup":"13","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":4625,"rowCount":50,"jobId":111,"name":"$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:95","description":"Delta: Job group for statement 13:\n# write the annotations dataframe to a delta lake table\nannotations_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'annotations'))\n\n# write the images dataframe to a delta lake table\nimages_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'images'))\n\n# write the categories dataframe to a delta lake table\ncategories_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'categories')): Compute snapshot for version: 4","submissionTime":"2023-04-20T06:49:25.930GMT","completionTime":"2023-04-20T06:49:25.956GMT","stageIds":[161,162,160],"jobGroup":"13","status":"SUCCEEDED","numTasks":56,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":55,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":4625,"dataRead":36167,"rowCount":120,"jobId":110,"name":"$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:95","description":"Delta: Job group for statement 13:\n# write the annotations dataframe to a delta lake table\nannotations_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'annotations'))\n\n# write the images dataframe to a delta lake table\nimages_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'images'))\n\n# write the categories dataframe to a delta lake table\ncategories_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'categories')): Compute snapshot for version: 4","submissionTime":"2023-04-20T06:49:25.728GMT","completionTime":"2023-04-20T06:49:25.916GMT","stageIds":[158,159],"jobGroup":"13","status":"SUCCEEDED","numTasks":55,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":5,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":36167,"dataRead":33146,"rowCount":140,"jobId":109,"name":"$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:95","description":"Delta: Job group for statement 13:\n# write the annotations dataframe to a delta lake table\nannotations_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'annotations'))\n\n# write the images dataframe to a delta lake table\nimages_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'images'))\n\n# write the categories dataframe to a delta lake table\ncategories_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'categories')): Compute snapshot for version: 4","submissionTime":"2023-04-20T06:49:25.228GMT","completionTime":"2023-04-20T06:49:25.595GMT","stageIds":[157],"jobGroup":"13","status":"SUCCEEDED","numTasks":5,"numActiveTasks":0,"numCompletedTasks":5,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":5,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":6493,"rowCount":28,"jobId":108,"name":"$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:95","description":"Job group for statement 13:\n# write the annotations dataframe to a delta lake table\nannotations_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'annotations'))\n\n# write the images dataframe to a delta lake table\nimages_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'images'))\n\n# write the categories dataframe to a delta lake table\ncategories_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'categories'))","submissionTime":"2023-04-20T06:49:23.293GMT","completionTime":"2023-04-20T06:49:23.414GMT","stageIds":[155,156],"jobGroup":"13","status":"SUCCEEDED","numTasks":54,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":4,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":183660092,"dataRead":175221620,"rowCount":13358078,"jobId":107,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 13:\n# write the annotations dataframe to a delta lake table\nannotations_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'annotations'))\n\n# write the images dataframe to a delta lake table\nimages_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'images'))\n\n# write the categories dataframe to a delta lake table\ncategories_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'categories'))","submissionTime":"2023-04-20T06:49:13.876GMT","completionTime":"2023-04-20T06:49:23.220GMT","stageIds":[154],"jobGroup":"13","status":"SUCCEEDED","numTasks":47,"numActiveTasks":0,"numCompletedTasks":47,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":47,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":4731,"rowCount":50,"jobId":106,"name":"$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:95","description":"Delta: Job group for statement 13:\n# write the annotations dataframe to a delta lake table\nannotations_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'annotations'))\n\n# write the images dataframe to a delta lake table\nimages_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'images'))\n\n# write the categories dataframe to a delta lake table\ncategories_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'categories')): Compute snapshot for version: 4","submissionTime":"2023-04-20T06:49:13.270GMT","completionTime":"2023-04-20T06:49:13.302GMT","stageIds":[153,151,152],"jobGroup":"13","status":"SUCCEEDED","numTasks":56,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":55,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":4731,"dataRead":46620,"rowCount":120,"jobId":105,"name":"$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:95","description":"Delta: Job group for statement 13:\n# write the annotations dataframe to a delta lake table\nannotations_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'annotations'))\n\n# write the images dataframe to a delta lake table\nimages_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'images'))\n\n# write the categories dataframe to a delta lake table\ncategories_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'categories')): Compute snapshot for version: 4","submissionTime":"2023-04-20T06:49:13.069GMT","completionTime":"2023-04-20T06:49:13.256GMT","stageIds":[150,149],"jobGroup":"13","status":"SUCCEEDED","numTasks":55,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":5,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":46620,"dataRead":52031,"rowCount":140,"jobId":104,"name":"$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:95","description":"Delta: Job group for statement 13:\n# write the annotations dataframe to a delta lake table\nannotations_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'annotations'))\n\n# write the images dataframe to a delta lake table\nimages_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'images'))\n\n# write the categories dataframe to a delta lake table\ncategories_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'categories')): Compute snapshot for version: 4","submissionTime":"2023-04-20T06:49:12.596GMT","completionTime":"2023-04-20T06:49:12.929GMT","stageIds":[148],"jobGroup":"13","status":"SUCCEEDED","numTasks":5,"numActiveTasks":0,"numCompletedTasks":5,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":5,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":7297,"rowCount":28,"jobId":103,"name":"$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:95","description":"Job group for statement 13:\n# write the annotations dataframe to a delta lake table\nannotations_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'annotations'))\n\n# write the images dataframe to a delta lake table\nimages_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'images'))\n\n# write the categories dataframe to a delta lake table\ncategories_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'categories'))","submissionTime":"2023-04-20T06:49:10.780GMT","completionTime":"2023-04-20T06:49:10.830GMT","stageIds":[147,146],"jobGroup":"13","status":"SUCCEEDED","numTasks":54,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":4,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":234586217,"dataRead":188741311,"rowCount":13510180,"jobId":102,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 13:\n# write the annotations dataframe to a delta lake table\nannotations_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'annotations'))\n\n# write the images dataframe to a delta lake table\nimages_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'images'))\n\n# write the categories dataframe to a delta lake table\ncategories_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'categories'))","submissionTime":"2023-04-20T06:48:56.583GMT","completionTime":"2023-04-20T06:49:10.632GMT","stageIds":[145],"jobGroup":"13","status":"SUCCEEDED","numTasks":49,"numActiveTasks":0,"numCompletedTasks":49,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":49,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"7072a564-abeb-40f9-a5e2-7e115cae5fea"},"text/plain":"StatementMeta(, 5d1aa8d0-49b9-4260-8dac-2ba686428313, 13, Finished, Available)"},"metadata":{}}],"execution_count":10,"metadata":{"advisor":{"adviceMetadata":"{\"artifactId\":\"76cad586-c48c-4c6d-b703-019950925c2e\",\"activityId\":\"5d1aa8d0-49b9-4260-8dac-2ba686428313\",\"applicationId\":\"application_1681971504383_0001\",\"jobGroupId\":\"13\",\"advices\":{\"info\":3,\"warn\":2}}"}},"id":"27262c23-bc84-4cc2-b060-ee7e31437160"},{"cell_type":"code","source":["# read and write S11 annotations and images\n","s11_annotations_df = spark.read.parquet(os.path.join(input_dir, s11_annotations_file_name))\n","s11_annotations_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 's11annotations'))\n","\n","s11_images_df = spark.read.parquet(os.path.join(input_dir, s11_images_file_name))\n","s11_images_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 's11images'))"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"5d1aa8d0-49b9-4260-8dac-2ba686428313","statement_id":16,"state":"finished","livy_statement_state":"available","queued_time":"2023-04-20T07:00:42.4154781Z","session_start_time":null,"execution_start_time":"2023-04-20T07:00:42.7239086Z","execution_finish_time":"2023-04-20T07:01:38.1236776Z","spark_jobs":{"numbers":{"RUNNING":0,"SUCCEEDED":12,"FAILED":0,"UNKNOWN":0},"jobs":[{"dataWritten":0,"dataRead":4400,"rowCount":50,"jobId":152,"name":"$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:95","description":"Delta: Job group for statement 16:\n# read and write S11 annotations and images\ns11_annotations_df = spark.read.parquet(os.path.join(input_dir, s11_annotations_file_name))\ns11_annotations_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 's11annotations'))\n\ns11_images_df = spark.read.parquet(os.path.join(input_dir, s11_images_file_name))\ns11_images_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 's11images')): Compute snapshot for version: 0","submissionTime":"2023-04-20T07:01:37.811GMT","completionTime":"2023-04-20T07:01:37.839GMT","stageIds":[219,217,218],"jobGroup":"16","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":51,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":4400,"dataRead":2136,"rowCount":55,"jobId":151,"name":"$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:95","description":"Delta: Job group for statement 16:\n# read and write S11 annotations and images\ns11_annotations_df = spark.read.parquet(os.path.join(input_dir, s11_annotations_file_name))\ns11_annotations_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 's11annotations'))\n\ns11_images_df = spark.read.parquet(os.path.join(input_dir, s11_images_file_name))\ns11_images_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 's11images')): Compute snapshot for version: 0","submissionTime":"2023-04-20T07:01:37.635GMT","completionTime":"2023-04-20T07:01:37.798GMT","stageIds":[215,216],"jobGroup":"16","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":2136,"dataRead":2220,"rowCount":10,"jobId":150,"name":"$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:95","description":"Delta: Job group for statement 16:\n# read and write S11 annotations and images\ns11_annotations_df = spark.read.parquet(os.path.join(input_dir, s11_annotations_file_name))\ns11_annotations_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 's11annotations'))\n\ns11_images_df = spark.read.parquet(os.path.join(input_dir, s11_images_file_name))\ns11_images_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 's11images')): Compute snapshot for version: 0","submissionTime":"2023-04-20T07:01:37.175GMT","completionTime":"2023-04-20T07:01:37.500GMT","stageIds":[214],"jobGroup":"16","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":149,"name":"","description":"Job group for statement 16:\n# read and write S11 annotations and images\ns11_annotations_df = spark.read.parquet(os.path.join(input_dir, s11_annotations_file_name))\ns11_annotations_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 's11annotations'))\n\ns11_images_df = spark.read.parquet(os.path.join(input_dir, s11_images_file_name))\ns11_images_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 's11images'))","submissionTime":"2023-04-20T07:01:31.868GMT","completionTime":"2023-04-20T07:01:31.868GMT","stageIds":[],"jobGroup":"16","status":"SUCCEEDED","numTasks":0,"numActiveTasks":0,"numCompletedTasks":0,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":0,"numActiveStages":0,"numCompletedStages":0,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":13414630,"dataRead":13101793,"rowCount":998802,"jobId":148,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 16:\n# read and write S11 annotations and images\ns11_annotations_df = spark.read.parquet(os.path.join(input_dir, s11_annotations_file_name))\ns11_annotations_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 's11annotations'))\n\ns11_images_df = spark.read.parquet(os.path.join(input_dir, s11_images_file_name))\ns11_images_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 's11images'))","submissionTime":"2023-04-20T07:01:25.189GMT","completionTime":"2023-04-20T07:01:31.751GMT","stageIds":[213],"jobGroup":"16","status":"SUCCEEDED","numTasks":4,"numActiveTasks":0,"numCompletedTasks":4,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":4,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":147,"name":"parquet at <unknown>:0","description":"Job group for statement 16:\n# read and write S11 annotations and images\ns11_annotations_df = spark.read.parquet(os.path.join(input_dir, s11_annotations_file_name))\ns11_annotations_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 's11annotations'))\n\ns11_images_df = spark.read.parquet(os.path.join(input_dir, s11_images_file_name))\ns11_images_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 's11images'))","submissionTime":"2023-04-20T07:01:23.750GMT","completionTime":"2023-04-20T07:01:24.213GMT","stageIds":[212],"jobGroup":"16","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":4502,"rowCount":50,"jobId":146,"name":"$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:95","description":"Delta: Job group for statement 16:\n# read and write S11 annotations and images\ns11_annotations_df = spark.read.parquet(os.path.join(input_dir, s11_annotations_file_name))\ns11_annotations_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 's11annotations'))\n\ns11_images_df = spark.read.parquet(os.path.join(input_dir, s11_images_file_name))\ns11_images_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 's11images')): Compute snapshot for version: 0","submissionTime":"2023-04-20T07:01:23.203GMT","completionTime":"2023-04-20T07:01:23.232GMT","stageIds":[209,210,211],"jobGroup":"16","status":"SUCCEEDED","numTasks":52,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":51,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":2,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":4502,"dataRead":2546,"rowCount":55,"jobId":145,"name":"$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:95","description":"Delta: Job group for statement 16:\n# read and write S11 annotations and images\ns11_annotations_df = spark.read.parquet(os.path.join(input_dir, s11_annotations_file_name))\ns11_annotations_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 's11annotations'))\n\ns11_images_df = spark.read.parquet(os.path.join(input_dir, s11_images_file_name))\ns11_images_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 's11images')): Compute snapshot for version: 0","submissionTime":"2023-04-20T07:01:23.022GMT","completionTime":"2023-04-20T07:01:23.188GMT","stageIds":[208,207],"jobGroup":"16","status":"SUCCEEDED","numTasks":51,"numActiveTasks":0,"numCompletedTasks":50,"numSkippedTasks":1,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":50,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":1,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":2546,"dataRead":3440,"rowCount":10,"jobId":144,"name":"$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:95","description":"Delta: Job group for statement 16:\n# read and write S11 annotations and images\ns11_annotations_df = spark.read.parquet(os.path.join(input_dir, s11_annotations_file_name))\ns11_annotations_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 's11annotations'))\n\ns11_images_df = spark.read.parquet(os.path.join(input_dir, s11_images_file_name))\ns11_images_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 's11images')): Compute snapshot for version: 0","submissionTime":"2023-04-20T07:01:22.556GMT","completionTime":"2023-04-20T07:01:22.900GMT","stageIds":[206],"jobGroup":"16","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":143,"name":"","description":"Job group for statement 16:\n# read and write S11 annotations and images\ns11_annotations_df = spark.read.parquet(os.path.join(input_dir, s11_annotations_file_name))\ns11_annotations_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 's11annotations'))\n\ns11_images_df = spark.read.parquet(os.path.join(input_dir, s11_images_file_name))\ns11_images_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 's11images'))","submissionTime":"2023-04-20T07:01:20.702GMT","completionTime":"2023-04-20T07:01:20.702GMT","stageIds":[],"jobGroup":"16","status":"SUCCEEDED","numTasks":0,"numActiveTasks":0,"numCompletedTasks":0,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":0,"numActiveStages":0,"numCompletedStages":0,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":17391704,"dataRead":14238314,"rowCount":1012910,"jobId":142,"name":"save at NativeMethodAccessorImpl.java:0","description":"Job group for statement 16:\n# read and write S11 annotations and images\ns11_annotations_df = spark.read.parquet(os.path.join(input_dir, s11_annotations_file_name))\ns11_annotations_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 's11annotations'))\n\ns11_images_df = spark.read.parquet(os.path.join(input_dir, s11_images_file_name))\ns11_images_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 's11images'))","submissionTime":"2023-04-20T07:01:11.402GMT","completionTime":"2023-04-20T07:01:20.624GMT","stageIds":[205],"jobGroup":"16","status":"SUCCEEDED","numTasks":4,"numActiveTasks":0,"numCompletedTasks":4,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":4,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}},{"dataWritten":0,"dataRead":0,"rowCount":0,"jobId":141,"name":"parquet at <unknown>:0","description":"Job group for statement 16:\n# read and write S11 annotations and images\ns11_annotations_df = spark.read.parquet(os.path.join(input_dir, s11_annotations_file_name))\ns11_annotations_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 's11annotations'))\n\ns11_images_df = spark.read.parquet(os.path.join(input_dir, s11_images_file_name))\ns11_images_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 's11images'))","submissionTime":"2023-04-20T07:01:09.222GMT","completionTime":"2023-04-20T07:01:10.472GMT","stageIds":[204],"jobGroup":"16","status":"SUCCEEDED","numTasks":1,"numActiveTasks":0,"numCompletedTasks":1,"numSkippedTasks":0,"numFailedTasks":0,"numKilledTasks":0,"numCompletedIndices":1,"numActiveStages":0,"numCompletedStages":1,"numSkippedStages":0,"numFailedStages":0,"killedTasksSummary":{}}],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"15da15f6-f7fe-4778-bf4d-3eb56e1c0d8b"},"text/plain":"StatementMeta(, 5d1aa8d0-49b9-4260-8dac-2ba686428313, 16, Finished, Available)"},"metadata":{}}],"execution_count":13,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"advisor":{"adviceMetadata":"{\"artifactId\":\"76cad586-c48c-4c6d-b703-019950925c2e\",\"activityId\":\"5d1aa8d0-49b9-4260-8dac-2ba686428313\",\"applicationId\":\"application_1681971504383_0001\",\"jobGroupId\":\"16\",\"advices\":{\"info\":2,\"warn\":2}}"}},"id":"e028a064-cdac-4833-9f8a-66b89488caff"},{"cell_type":"code","source":[],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"4be1127a-0b7b-4dcf-855d-e2f4fcde98e7"}],"metadata":{"language_info":{"name":"python"},"kernelspec":{"name":"synapse_pyspark","display_name":"Synapse PySpark"},"widgets":{},"kernel_info":{"name":"synapse_pyspark"},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"enableDebugMode":false,"conf":{}}},"notebook_environment":{},"synapse_widget":{"version":"0.1","state":{}},"trident":{"lakehouse":{"default_lakehouse":"5f58e739-d87b-404a-9a7b-5e64738a82c5","known_lakehouses":[{"id":"5f58e739-d87b-404a-9a7b-5e64738a82c5"}],"default_lakehouse_name":"Serengeti_Lakehouse","default_lakehouse_workspace_id":"1eaa972c-c94a-47fa-90f5-bcad0a19f945"}}},"nbformat":4,"nbformat_minor":5}