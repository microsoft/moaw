{"cells":[{"cell_type":"code","execution_count":3,"id":"7960e481-fe2a-463f-8118-7cad08f2c191","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2023-04-20T06:44:09.8363963Z","execution_start_time":"2023-04-20T06:44:09.4082676Z","livy_statement_state":"available","parent_msg_id":"901d891f-e229-43f3-ba8e-a558959907db","queued_time":"2023-04-20T06:44:08.5492385Z","session_id":"5d1aa8d0-49b9-4260-8dac-2ba686428313","session_start_time":"2023-04-20T06:44:09.2362833Z","spark_jobs":{"jobs":[],"limit":20,"numbers":{"FAILED":0,"RUNNING":0,"SUCCEEDED":0,"UNKNOWN":0},"rule":"ALL_DESC"},"spark_pool":null,"state":"finished","statement_id":6},"text/plain":["StatementMeta(, 5d1aa8d0-49b9-4260-8dac-2ba686428313, 6, Finished, Available)"]},"metadata":{},"output_type":"display_data"}],"source":["import os"]},{"cell_type":"code","execution_count":4,"id":"c71bc4ba-ae14-4f00-882d-13aba7555452","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2023-04-20T06:44:10.5564444Z","execution_start_time":"2023-04-20T06:44:10.2004307Z","livy_statement_state":"available","parent_msg_id":"a1c0a454-5d4f-4743-a0bb-e80f5cb8290b","queued_time":"2023-04-20T06:44:08.5546358Z","session_id":"5d1aa8d0-49b9-4260-8dac-2ba686428313","session_start_time":null,"spark_jobs":{"jobs":[],"limit":20,"numbers":{"FAILED":0,"RUNNING":0,"SUCCEEDED":0,"UNKNOWN":0},"rule":"ALL_DESC"},"spark_pool":null,"state":"finished","statement_id":7},"text/plain":["StatementMeta(, 5d1aa8d0-49b9-4260-8dac-2ba686428313, 7, Finished, Available)"]},"metadata":{},"output_type":"display_data"}],"source":["# Annotations files S01 to S10\n","annotations_file_names = ['Annotations-S01.parquet', 'Annotations-S02.parquet', 'Annotations-S03.parquet', 'Annotations-S04.parquet', 'Annotations-S05.parquet', 'Annotations-S06.parquet', 'Annotations-S07.parquet', 'Annotations-S08.parquet', 'Annotations-S09.parquet', 'Annotations-S10.parquet']\n","\n","# Images files S01 to S10\n","images_file_names = ['Images-S01.parquet', 'Images-S02.parquet', 'Images-S03.parquet', 'Images-S04.parquet', 'Images-S05.parquet', 'Images-S06.parquet', 'Images-S07.parquet', 'Images-S08.parquet', 'Images-S09.parquet', 'Images-S10.parquet']\n","\n","# S11 files\n","s11_annotations_file_name = 'Annotations-S11.parquet'\n","s11_images_file_name = 'Images-S11.parquet'\n","\n","# Categories file\n","categories_file = 'Categories.parquet'\n","\n","# file paths\n","input_dir = 'Files/metadata/parquet'\n","output_dir = 'Files/metadata/delta-lake'"]},{"cell_type":"code","execution_count":7,"id":"c5f6a001-4ed6-4fdb-abf0-cc71df93ff4b","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2023-04-20T06:46:53.1462521Z","execution_start_time":"2023-04-20T06:46:20.2847115Z","livy_statement_state":"available","parent_msg_id":"e856f324-d414-4db2-8083-972fd288f1b9","queued_time":"2023-04-20T06:46:19.958477Z","session_id":"5d1aa8d0-49b9-4260-8dac-2ba686428313","session_start_time":null,"spark_jobs":{"jobs":[{"completionTime":"2023-04-20T06:46:51.231GMT","dataRead":59,"dataWritten":0,"description":"Job group for statement 10:\n# read the annotations parquet files using spark and append as one dataframe\nannotations_df = spark.read.parquet(os.path.join(input_dir, annotations_file_names[0]))\nfor file_name in annotations_file_names[1:]:\n    annotations_df = annotations_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\nprint(f'annotations : {annotations_df.count()}')\n\n# read the images parquet files using spark and append as one dataframe\nimages_df = spark.read.parquet(os.path.join(input_dir, images_file_names[0]))\nfor file_name in images_file_names[1:]:\n    images_df = images_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\nprint(f'images : {images_df.count()}')\n\n# read the categories parquet file using spark\ncategories_df = spark.read.parquet(os.path.join(input_dir, categories_file))\nprint(f'categories : {categories_df.count()}')","jobGroup":"10","jobId":65,"killedTasksSummary":{},"name":"count at NativeMethodAccessorImpl.java:0","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":1,"numTasks":2,"rowCount":1,"stageIds":[78,79],"status":"SUCCEEDED","submissionTime":"2023-04-20T06:46:51.204GMT"},{"completionTime":"2023-04-20T06:46:51.194GMT","dataRead":572,"dataWritten":59,"description":"Job group for statement 10:\n# read the annotations parquet files using spark and append as one dataframe\nannotations_df = spark.read.parquet(os.path.join(input_dir, annotations_file_names[0]))\nfor file_name in annotations_file_names[1:]:\n    annotations_df = annotations_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\nprint(f'annotations : {annotations_df.count()}')\n\n# read the images parquet files using spark and append as one dataframe\nimages_df = spark.read.parquet(os.path.join(input_dir, images_file_names[0]))\nfor file_name in images_file_names[1:]:\n    images_df = images_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\nprint(f'images : {images_df.count()}')\n\n# read the categories parquet file using spark\ncategories_df = spark.read.parquet(os.path.join(input_dir, categories_file))\nprint(f'categories : {categories_df.count()}')","jobGroup":"10","jobId":64,"killedTasksSummary":{},"name":"count at NativeMethodAccessorImpl.java:0","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":1,"rowCount":62,"stageIds":[77],"status":"SUCCEEDED","submissionTime":"2023-04-20T06:46:50.142GMT"},{"completionTime":"2023-04-20T06:46:50.048GMT","dataRead":0,"dataWritten":0,"description":"Job group for statement 10:\n# read the annotations parquet files using spark and append as one dataframe\nannotations_df = spark.read.parquet(os.path.join(input_dir, annotations_file_names[0]))\nfor file_name in annotations_file_names[1:]:\n    annotations_df = annotations_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\nprint(f'annotations : {annotations_df.count()}')\n\n# read the images parquet files using spark and append as one dataframe\nimages_df = spark.read.parquet(os.path.join(input_dir, images_file_names[0]))\nfor file_name in images_file_names[1:]:\n    images_df = images_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\nprint(f'images : {images_df.count()}')\n\n# read the categories parquet file using spark\ncategories_df = spark.read.parquet(os.path.join(input_dir, categories_file))\nprint(f'categories : {categories_df.count()}')","jobGroup":"10","jobId":63,"killedTasksSummary":{},"name":"parquet at <unknown>:0","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":1,"rowCount":0,"stageIds":[76],"status":"SUCCEEDED","submissionTime":"2023-04-20T06:46:49.604GMT"},{"completionTime":"2023-04-20T06:46:49.067GMT","dataRead":2662,"dataWritten":0,"description":"Job group for statement 10:\n# read the annotations parquet files using spark and append as one dataframe\nannotations_df = spark.read.parquet(os.path.join(input_dir, annotations_file_names[0]))\nfor file_name in annotations_file_names[1:]:\n    annotations_df = annotations_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\nprint(f'annotations : {annotations_df.count()}')\n\n# read the images parquet files using spark and append as one dataframe\nimages_df = spark.read.parquet(os.path.join(input_dir, images_file_names[0]))\nfor file_name in images_file_names[1:]:\n    images_df = images_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\nprint(f'images : {images_df.count()}')\n\n# read the categories parquet file using spark\ncategories_df = spark.read.parquet(os.path.join(input_dir, categories_file))\nprint(f'categories : {categories_df.count()}')","jobGroup":"10","jobId":62,"killedTasksSummary":{},"name":"count at NativeMethodAccessorImpl.java:0","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":47,"numTasks":48,"rowCount":47,"stageIds":[74,75],"status":"SUCCEEDED","submissionTime":"2023-04-20T06:46:49.040GMT"},{"completionTime":"2023-04-20T06:46:49.026GMT","dataRead":68315,"dataWritten":2662,"description":"Job group for statement 10:\n# read the annotations parquet files using spark and append as one dataframe\nannotations_df = spark.read.parquet(os.path.join(input_dir, annotations_file_names[0]))\nfor file_name in annotations_file_names[1:]:\n    annotations_df = annotations_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\nprint(f'annotations : {annotations_df.count()}')\n\n# read the images parquet files using spark and append as one dataframe\nimages_df = spark.read.parquet(os.path.join(input_dir, images_file_names[0]))\nfor file_name in images_file_names[1:]:\n    images_df = images_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\nprint(f'images : {images_df.count()}')\n\n# read the categories parquet file using spark\ncategories_df = spark.read.parquet(os.path.join(input_dir, categories_file))\nprint(f'categories : {categories_df.count()}')","jobGroup":"10","jobId":61,"killedTasksSummary":{},"name":"count at NativeMethodAccessorImpl.java:0","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":47,"numCompletedStages":1,"numCompletedTasks":47,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":47,"rowCount":6679086,"stageIds":[73],"status":"SUCCEEDED","submissionTime":"2023-04-20T06:46:45.582GMT"},{"completionTime":"2023-04-20T06:46:45.301GMT","dataRead":0,"dataWritten":0,"description":"Job group for statement 10:\n# read the annotations parquet files using spark and append as one dataframe\nannotations_df = spark.read.parquet(os.path.join(input_dir, annotations_file_names[0]))\nfor file_name in annotations_file_names[1:]:\n    annotations_df = annotations_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\nprint(f'annotations : {annotations_df.count()}')\n\n# read the images parquet files using spark and append as one dataframe\nimages_df = spark.read.parquet(os.path.join(input_dir, images_file_names[0]))\nfor file_name in images_file_names[1:]:\n    images_df = images_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\nprint(f'images : {images_df.count()}')\n\n# read the categories parquet file using spark\ncategories_df = spark.read.parquet(os.path.join(input_dir, categories_file))\nprint(f'categories : {categories_df.count()}')","jobGroup":"10","jobId":60,"killedTasksSummary":{},"name":"parquet at <unknown>:0","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":1,"rowCount":0,"stageIds":[72],"status":"SUCCEEDED","submissionTime":"2023-04-20T06:46:44.825GMT"},{"completionTime":"2023-04-20T06:46:44.347GMT","dataRead":0,"dataWritten":0,"description":"Job group for statement 10:\n# read the annotations parquet files using spark and append as one dataframe\nannotations_df = spark.read.parquet(os.path.join(input_dir, annotations_file_names[0]))\nfor file_name in annotations_file_names[1:]:\n    annotations_df = annotations_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\nprint(f'annotations : {annotations_df.count()}')\n\n# read the images parquet files using spark and append as one dataframe\nimages_df = spark.read.parquet(os.path.join(input_dir, images_file_names[0]))\nfor file_name in images_file_names[1:]:\n    images_df = images_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\nprint(f'images : {images_df.count()}')\n\n# read the categories parquet file using spark\ncategories_df = spark.read.parquet(os.path.join(input_dir, categories_file))\nprint(f'categories : {categories_df.count()}')","jobGroup":"10","jobId":59,"killedTasksSummary":{},"name":"parquet at <unknown>:0","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":1,"rowCount":0,"stageIds":[71],"status":"SUCCEEDED","submissionTime":"2023-04-20T06:46:43.894GMT"},{"completionTime":"2023-04-20T06:46:43.446GMT","dataRead":0,"dataWritten":0,"description":"Job group for statement 10:\n# read the annotations parquet files using spark and append as one dataframe\nannotations_df = spark.read.parquet(os.path.join(input_dir, annotations_file_names[0]))\nfor file_name in annotations_file_names[1:]:\n    annotations_df = annotations_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\nprint(f'annotations : {annotations_df.count()}')\n\n# read the images parquet files using spark and append as one dataframe\nimages_df = spark.read.parquet(os.path.join(input_dir, images_file_names[0]))\nfor file_name in images_file_names[1:]:\n    images_df = images_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\nprint(f'images : {images_df.count()}')\n\n# read the categories parquet file using spark\ncategories_df = spark.read.parquet(os.path.join(input_dir, categories_file))\nprint(f'categories : {categories_df.count()}')","jobGroup":"10","jobId":58,"killedTasksSummary":{},"name":"parquet at <unknown>:0","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":1,"rowCount":0,"stageIds":[70],"status":"SUCCEEDED","submissionTime":"2023-04-20T06:46:42.984GMT"},{"completionTime":"2023-04-20T06:46:42.460GMT","dataRead":0,"dataWritten":0,"description":"Job group for statement 10:\n# read the annotations parquet files using spark and append as one dataframe\nannotations_df = spark.read.parquet(os.path.join(input_dir, annotations_file_names[0]))\nfor file_name in annotations_file_names[1:]:\n    annotations_df = annotations_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\nprint(f'annotations : {annotations_df.count()}')\n\n# read the images parquet files using spark and append as one dataframe\nimages_df = spark.read.parquet(os.path.join(input_dir, images_file_names[0]))\nfor file_name in images_file_names[1:]:\n    images_df = images_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\nprint(f'images : {images_df.count()}')\n\n# read the categories parquet file using spark\ncategories_df = spark.read.parquet(os.path.join(input_dir, categories_file))\nprint(f'categories : {categories_df.count()}')","jobGroup":"10","jobId":57,"killedTasksSummary":{},"name":"parquet at <unknown>:0","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":1,"rowCount":0,"stageIds":[69],"status":"SUCCEEDED","submissionTime":"2023-04-20T06:46:42.030GMT"},{"completionTime":"2023-04-20T06:46:41.524GMT","dataRead":0,"dataWritten":0,"description":"Job group for statement 10:\n# read the annotations parquet files using spark and append as one dataframe\nannotations_df = spark.read.parquet(os.path.join(input_dir, annotations_file_names[0]))\nfor file_name in annotations_file_names[1:]:\n    annotations_df = annotations_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\nprint(f'annotations : {annotations_df.count()}')\n\n# read the images parquet files using spark and append as one dataframe\nimages_df = spark.read.parquet(os.path.join(input_dir, images_file_names[0]))\nfor file_name in images_file_names[1:]:\n    images_df = images_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\nprint(f'images : {images_df.count()}')\n\n# read the categories parquet file using spark\ncategories_df = spark.read.parquet(os.path.join(input_dir, categories_file))\nprint(f'categories : {categories_df.count()}')","jobGroup":"10","jobId":56,"killedTasksSummary":{},"name":"parquet at <unknown>:0","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":1,"rowCount":0,"stageIds":[68],"status":"SUCCEEDED","submissionTime":"2023-04-20T06:46:41.088GMT"},{"completionTime":"2023-04-20T06:46:40.621GMT","dataRead":0,"dataWritten":0,"description":"Job group for statement 10:\n# read the annotations parquet files using spark and append as one dataframe\nannotations_df = spark.read.parquet(os.path.join(input_dir, annotations_file_names[0]))\nfor file_name in annotations_file_names[1:]:\n    annotations_df = annotations_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\nprint(f'annotations : {annotations_df.count()}')\n\n# read the images parquet files using spark and append as one dataframe\nimages_df = spark.read.parquet(os.path.join(input_dir, images_file_names[0]))\nfor file_name in images_file_names[1:]:\n    images_df = images_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\nprint(f'images : {images_df.count()}')\n\n# read the categories parquet file using spark\ncategories_df = spark.read.parquet(os.path.join(input_dir, categories_file))\nprint(f'categories : {categories_df.count()}')","jobGroup":"10","jobId":55,"killedTasksSummary":{},"name":"parquet at <unknown>:0","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":1,"rowCount":0,"stageIds":[67],"status":"SUCCEEDED","submissionTime":"2023-04-20T06:46:40.200GMT"},{"completionTime":"2023-04-20T06:46:39.739GMT","dataRead":0,"dataWritten":0,"description":"Job group for statement 10:\n# read the annotations parquet files using spark and append as one dataframe\nannotations_df = spark.read.parquet(os.path.join(input_dir, annotations_file_names[0]))\nfor file_name in annotations_file_names[1:]:\n    annotations_df = annotations_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\nprint(f'annotations : {annotations_df.count()}')\n\n# read the images parquet files using spark and append as one dataframe\nimages_df = spark.read.parquet(os.path.join(input_dir, images_file_names[0]))\nfor file_name in images_file_names[1:]:\n    images_df = images_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\nprint(f'images : {images_df.count()}')\n\n# read the categories parquet file using spark\ncategories_df = spark.read.parquet(os.path.join(input_dir, categories_file))\nprint(f'categories : {categories_df.count()}')","jobGroup":"10","jobId":54,"killedTasksSummary":{},"name":"parquet at <unknown>:0","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":1,"rowCount":0,"stageIds":[66],"status":"SUCCEEDED","submissionTime":"2023-04-20T06:46:39.177GMT"},{"completionTime":"2023-04-20T06:46:38.685GMT","dataRead":0,"dataWritten":0,"description":"Job group for statement 10:\n# read the annotations parquet files using spark and append as one dataframe\nannotations_df = spark.read.parquet(os.path.join(input_dir, annotations_file_names[0]))\nfor file_name in annotations_file_names[1:]:\n    annotations_df = annotations_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\nprint(f'annotations : {annotations_df.count()}')\n\n# read the images parquet files using spark and append as one dataframe\nimages_df = spark.read.parquet(os.path.join(input_dir, images_file_names[0]))\nfor file_name in images_file_names[1:]:\n    images_df = images_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\nprint(f'images : {images_df.count()}')\n\n# read the categories parquet file using spark\ncategories_df = spark.read.parquet(os.path.join(input_dir, categories_file))\nprint(f'categories : {categories_df.count()}')","jobGroup":"10","jobId":53,"killedTasksSummary":{},"name":"parquet at <unknown>:0","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":1,"rowCount":0,"stageIds":[65],"status":"SUCCEEDED","submissionTime":"2023-04-20T06:46:38.236GMT"},{"completionTime":"2023-04-20T06:46:37.713GMT","dataRead":0,"dataWritten":0,"description":"Job group for statement 10:\n# read the annotations parquet files using spark and append as one dataframe\nannotations_df = spark.read.parquet(os.path.join(input_dir, annotations_file_names[0]))\nfor file_name in annotations_file_names[1:]:\n    annotations_df = annotations_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\nprint(f'annotations : {annotations_df.count()}')\n\n# read the images parquet files using spark and append as one dataframe\nimages_df = spark.read.parquet(os.path.join(input_dir, images_file_names[0]))\nfor file_name in images_file_names[1:]:\n    images_df = images_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\nprint(f'images : {images_df.count()}')\n\n# read the categories parquet file using spark\ncategories_df = spark.read.parquet(os.path.join(input_dir, categories_file))\nprint(f'categories : {categories_df.count()}')","jobGroup":"10","jobId":52,"killedTasksSummary":{},"name":"parquet at <unknown>:0","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":1,"rowCount":0,"stageIds":[64],"status":"SUCCEEDED","submissionTime":"2023-04-20T06:46:37.260GMT"},{"completionTime":"2023-04-20T06:46:36.776GMT","dataRead":0,"dataWritten":0,"description":"Job group for statement 10:\n# read the annotations parquet files using spark and append as one dataframe\nannotations_df = spark.read.parquet(os.path.join(input_dir, annotations_file_names[0]))\nfor file_name in annotations_file_names[1:]:\n    annotations_df = annotations_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\nprint(f'annotations : {annotations_df.count()}')\n\n# read the images parquet files using spark and append as one dataframe\nimages_df = spark.read.parquet(os.path.join(input_dir, images_file_names[0]))\nfor file_name in images_file_names[1:]:\n    images_df = images_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\nprint(f'images : {images_df.count()}')\n\n# read the categories parquet file using spark\ncategories_df = spark.read.parquet(os.path.join(input_dir, categories_file))\nprint(f'categories : {categories_df.count()}')","jobGroup":"10","jobId":51,"killedTasksSummary":{},"name":"parquet at <unknown>:0","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":1,"rowCount":0,"stageIds":[63],"status":"SUCCEEDED","submissionTime":"2023-04-20T06:46:36.306GMT"},{"completionTime":"2023-04-20T06:46:35.829GMT","dataRead":2774,"dataWritten":0,"description":"Job group for statement 10:\n# read the annotations parquet files using spark and append as one dataframe\nannotations_df = spark.read.parquet(os.path.join(input_dir, annotations_file_names[0]))\nfor file_name in annotations_file_names[1:]:\n    annotations_df = annotations_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\nprint(f'annotations : {annotations_df.count()}')\n\n# read the images parquet files using spark and append as one dataframe\nimages_df = spark.read.parquet(os.path.join(input_dir, images_file_names[0]))\nfor file_name in images_file_names[1:]:\n    images_df = images_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\nprint(f'images : {images_df.count()}')\n\n# read the categories parquet file using spark\ncategories_df = spark.read.parquet(os.path.join(input_dir, categories_file))\nprint(f'categories : {categories_df.count()}')","jobGroup":"10","jobId":50,"killedTasksSummary":{},"name":"count at NativeMethodAccessorImpl.java:0","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":49,"numTasks":50,"rowCount":49,"stageIds":[61,62],"status":"SUCCEEDED","submissionTime":"2023-04-20T06:46:35.793GMT"},{"completionTime":"2023-04-20T06:46:35.780GMT","dataRead":91281,"dataWritten":2774,"description":"Job group for statement 10:\n# read the annotations parquet files using spark and append as one dataframe\nannotations_df = spark.read.parquet(os.path.join(input_dir, annotations_file_names[0]))\nfor file_name in annotations_file_names[1:]:\n    annotations_df = annotations_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\nprint(f'annotations : {annotations_df.count()}')\n\n# read the images parquet files using spark and append as one dataframe\nimages_df = spark.read.parquet(os.path.join(input_dir, images_file_names[0]))\nfor file_name in images_file_names[1:]:\n    images_df = images_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\nprint(f'images : {images_df.count()}')\n\n# read the categories parquet file using spark\ncategories_df = spark.read.parquet(os.path.join(input_dir, categories_file))\nprint(f'categories : {categories_df.count()}')","jobGroup":"10","jobId":49,"killedTasksSummary":{},"name":"count at NativeMethodAccessorImpl.java:0","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":49,"numCompletedStages":1,"numCompletedTasks":49,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":49,"rowCount":6755139,"stageIds":[60],"status":"SUCCEEDED","submissionTime":"2023-04-20T06:46:32.370GMT"},{"completionTime":"2023-04-20T06:46:32.087GMT","dataRead":0,"dataWritten":0,"description":"Job group for statement 10:\n# read the annotations parquet files using spark and append as one dataframe\nannotations_df = spark.read.parquet(os.path.join(input_dir, annotations_file_names[0]))\nfor file_name in annotations_file_names[1:]:\n    annotations_df = annotations_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\nprint(f'annotations : {annotations_df.count()}')\n\n# read the images parquet files using spark and append as one dataframe\nimages_df = spark.read.parquet(os.path.join(input_dir, images_file_names[0]))\nfor file_name in images_file_names[1:]:\n    images_df = images_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\nprint(f'images : {images_df.count()}')\n\n# read the categories parquet file using spark\ncategories_df = spark.read.parquet(os.path.join(input_dir, categories_file))\nprint(f'categories : {categories_df.count()}')","jobGroup":"10","jobId":48,"killedTasksSummary":{},"name":"parquet at <unknown>:0","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":1,"rowCount":0,"stageIds":[59],"status":"SUCCEEDED","submissionTime":"2023-04-20T06:46:31.617GMT"},{"completionTime":"2023-04-20T06:46:31.156GMT","dataRead":0,"dataWritten":0,"description":"Job group for statement 10:\n# read the annotations parquet files using spark and append as one dataframe\nannotations_df = spark.read.parquet(os.path.join(input_dir, annotations_file_names[0]))\nfor file_name in annotations_file_names[1:]:\n    annotations_df = annotations_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\nprint(f'annotations : {annotations_df.count()}')\n\n# read the images parquet files using spark and append as one dataframe\nimages_df = spark.read.parquet(os.path.join(input_dir, images_file_names[0]))\nfor file_name in images_file_names[1:]:\n    images_df = images_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\nprint(f'images : {images_df.count()}')\n\n# read the categories parquet file using spark\ncategories_df = spark.read.parquet(os.path.join(input_dir, categories_file))\nprint(f'categories : {categories_df.count()}')","jobGroup":"10","jobId":47,"killedTasksSummary":{},"name":"parquet at <unknown>:0","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":1,"rowCount":0,"stageIds":[58],"status":"SUCCEEDED","submissionTime":"2023-04-20T06:46:30.732GMT"},{"completionTime":"2023-04-20T06:46:30.264GMT","dataRead":0,"dataWritten":0,"description":"Job group for statement 10:\n# read the annotations parquet files using spark and append as one dataframe\nannotations_df = spark.read.parquet(os.path.join(input_dir, annotations_file_names[0]))\nfor file_name in annotations_file_names[1:]:\n    annotations_df = annotations_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\nprint(f'annotations : {annotations_df.count()}')\n\n# read the images parquet files using spark and append as one dataframe\nimages_df = spark.read.parquet(os.path.join(input_dir, images_file_names[0]))\nfor file_name in images_file_names[1:]:\n    images_df = images_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\nprint(f'images : {images_df.count()}')\n\n# read the categories parquet file using spark\ncategories_df = spark.read.parquet(os.path.join(input_dir, categories_file))\nprint(f'categories : {categories_df.count()}')","jobGroup":"10","jobId":46,"killedTasksSummary":{},"name":"parquet at <unknown>:0","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":1,"rowCount":0,"stageIds":[57],"status":"SUCCEEDED","submissionTime":"2023-04-20T06:46:29.828GMT"}],"limit":20,"numbers":{"FAILED":0,"RUNNING":0,"SUCCEEDED":27,"UNKNOWN":0},"rule":"ALL_DESC"},"spark_pool":null,"state":"finished","statement_id":10},"text/plain":["StatementMeta(, 5d1aa8d0-49b9-4260-8dac-2ba686428313, 10, Finished, Available)"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["annotations : 6755090\n","images : 6679039\n","categories : 61\n"]}],"source":["# read the annotations parquet files using spark and append as one dataframe\n","annotations_df = spark.read.parquet(os.path.join(input_dir, annotations_file_names[0]))\n","for file_name in annotations_file_names[1:]:\n","    annotations_df = annotations_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\n","print(f'annotations : {annotations_df.count()}')\n","\n","# read the images parquet files using spark and append as one dataframe\n","images_df = spark.read.parquet(os.path.join(input_dir, images_file_names[0]))\n","for file_name in images_file_names[1:]:\n","    images_df = images_df.union(spark.read.parquet(os.path.join(input_dir, file_name)))\n","print(f'images : {images_df.count()}')\n","\n","# read the categories parquet file using spark\n","categories_df = spark.read.parquet(os.path.join(input_dir, categories_file))\n","print(f'categories : {categories_df.count()}')"]},{"cell_type":"code","execution_count":10,"id":"27262c23-bc84-4cc2-b060-ee7e31437160","metadata":{"advisor":{"adviceMetadata":"{\"artifactId\":\"76cad586-c48c-4c6d-b703-019950925c2e\",\"activityId\":\"5d1aa8d0-49b9-4260-8dac-2ba686428313\",\"applicationId\":\"application_1681971504383_0001\",\"jobGroupId\":\"13\",\"advices\":{\"info\":3,\"warn\":2}}"}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2023-04-20T06:49:33.5361004Z","execution_start_time":"2023-04-20T06:48:56.0265366Z","livy_statement_state":"available","parent_msg_id":"7072a564-abeb-40f9-a5e2-7e115cae5fea","queued_time":"2023-04-20T06:48:55.7347141Z","session_id":"5d1aa8d0-49b9-4260-8dac-2ba686428313","session_start_time":null,"spark_jobs":{"jobs":[{"completionTime":"2023-04-20T06:49:31.649GMT","dataRead":4322,"dataWritten":0,"description":"Delta: Job group for statement 13:\n# write the annotations dataframe to a delta lake table\nannotations_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'annotations'))\n\n# write the images dataframe to a delta lake table\nimages_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'images'))\n\n# write the categories dataframe to a delta lake table\ncategories_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'categories')): Compute snapshot for version: 2","jobGroup":"13","jobId":116,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":2,"numSkippedTasks":53,"numTasks":54,"rowCount":50,"stageIds":[171,169,170],"status":"SUCCEEDED","submissionTime":"2023-04-20T06:49:31.619GMT"},{"completionTime":"2023-04-20T06:49:31.600GMT","dataRead":4639,"dataWritten":4322,"description":"Delta: Job group for statement 13:\n# write the annotations dataframe to a delta lake table\nannotations_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'annotations'))\n\n# write the images dataframe to a delta lake table\nimages_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'images'))\n\n# write the categories dataframe to a delta lake table\ncategories_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'categories')): Compute snapshot for version: 2","jobGroup":"13","jobId":115,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":3,"numTasks":53,"rowCount":60,"stageIds":[168,167],"status":"SUCCEEDED","submissionTime":"2023-04-20T06:49:31.417GMT"},{"completionTime":"2023-04-20T06:49:31.293GMT","dataRead":3179,"dataWritten":4639,"description":"Delta: Job group for statement 13:\n# write the annotations dataframe to a delta lake table\nannotations_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'annotations'))\n\n# write the images dataframe to a delta lake table\nimages_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'images'))\n\n# write the categories dataframe to a delta lake table\ncategories_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'categories')): Compute snapshot for version: 2","jobGroup":"13","jobId":114,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":3,"numCompletedStages":1,"numCompletedTasks":3,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":3,"rowCount":20,"stageIds":[166],"status":"SUCCEEDED","submissionTime":"2023-04-20T06:49:30.935GMT"},{"completionTime":"2023-04-20T06:49:29.367GMT","dataRead":1879,"dataWritten":0,"description":"Job group for statement 13:\n# write the annotations dataframe to a delta lake table\nannotations_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'annotations'))\n\n# write the images dataframe to a delta lake table\nimages_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'images'))\n\n# write the categories dataframe to a delta lake table\ncategories_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'categories'))","jobGroup":"13","jobId":113,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":2,"numTasks":52,"rowCount":4,"stageIds":[165,164],"status":"SUCCEEDED","submissionTime":"2023-04-20T06:49:29.319GMT"},{"completionTime":"2023-04-20T06:49:29.253GMT","dataRead":2333,"dataWritten":2163,"description":"Job group for statement 13:\n# write the annotations dataframe to a delta lake table\nannotations_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'annotations'))\n\n# write the images dataframe to a delta lake table\nimages_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'images'))\n\n# write the categories dataframe to a delta lake table\ncategories_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'categories'))","jobGroup":"13","jobId":112,"killedTasksSummary":{},"name":"save at NativeMethodAccessorImpl.java:0","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":1,"rowCount":122,"stageIds":[163],"status":"SUCCEEDED","submissionTime":"2023-04-20T06:49:26.420GMT"},{"completionTime":"2023-04-20T06:49:25.956GMT","dataRead":4625,"dataWritten":0,"description":"Delta: Job group for statement 13:\n# write the annotations dataframe to a delta lake table\nannotations_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'annotations'))\n\n# write the images dataframe to a delta lake table\nimages_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'images'))\n\n# write the categories dataframe to a delta lake table\ncategories_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'categories')): Compute snapshot for version: 4","jobGroup":"13","jobId":111,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":2,"numSkippedTasks":55,"numTasks":56,"rowCount":50,"stageIds":[161,162,160],"status":"SUCCEEDED","submissionTime":"2023-04-20T06:49:25.930GMT"},{"completionTime":"2023-04-20T06:49:25.916GMT","dataRead":36167,"dataWritten":4625,"description":"Delta: Job group for statement 13:\n# write the annotations dataframe to a delta lake table\nannotations_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'annotations'))\n\n# write the images dataframe to a delta lake table\nimages_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'images'))\n\n# write the categories dataframe to a delta lake table\ncategories_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'categories')): Compute snapshot for version: 4","jobGroup":"13","jobId":110,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":5,"numTasks":55,"rowCount":120,"stageIds":[158,159],"status":"SUCCEEDED","submissionTime":"2023-04-20T06:49:25.728GMT"},{"completionTime":"2023-04-20T06:49:25.595GMT","dataRead":33146,"dataWritten":36167,"description":"Delta: Job group for statement 13:\n# write the annotations dataframe to a delta lake table\nannotations_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'annotations'))\n\n# write the images dataframe to a delta lake table\nimages_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'images'))\n\n# write the categories dataframe to a delta lake table\ncategories_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'categories')): Compute snapshot for version: 4","jobGroup":"13","jobId":109,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":5,"numCompletedStages":1,"numCompletedTasks":5,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":5,"rowCount":140,"stageIds":[157],"status":"SUCCEEDED","submissionTime":"2023-04-20T06:49:25.228GMT"},{"completionTime":"2023-04-20T06:49:23.414GMT","dataRead":6493,"dataWritten":0,"description":"Job group for statement 13:\n# write the annotations dataframe to a delta lake table\nannotations_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'annotations'))\n\n# write the images dataframe to a delta lake table\nimages_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'images'))\n\n# write the categories dataframe to a delta lake table\ncategories_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'categories'))","jobGroup":"13","jobId":108,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":4,"numTasks":54,"rowCount":28,"stageIds":[155,156],"status":"SUCCEEDED","submissionTime":"2023-04-20T06:49:23.293GMT"},{"completionTime":"2023-04-20T06:49:23.220GMT","dataRead":175221620,"dataWritten":183660092,"description":"Job group for statement 13:\n# write the annotations dataframe to a delta lake table\nannotations_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'annotations'))\n\n# write the images dataframe to a delta lake table\nimages_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'images'))\n\n# write the categories dataframe to a delta lake table\ncategories_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'categories'))","jobGroup":"13","jobId":107,"killedTasksSummary":{},"name":"save at NativeMethodAccessorImpl.java:0","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":47,"numCompletedStages":1,"numCompletedTasks":47,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":47,"rowCount":13358078,"stageIds":[154],"status":"SUCCEEDED","submissionTime":"2023-04-20T06:49:13.876GMT"},{"completionTime":"2023-04-20T06:49:13.302GMT","dataRead":4731,"dataWritten":0,"description":"Delta: Job group for statement 13:\n# write the annotations dataframe to a delta lake table\nannotations_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'annotations'))\n\n# write the images dataframe to a delta lake table\nimages_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'images'))\n\n# write the categories dataframe to a delta lake table\ncategories_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'categories')): Compute snapshot for version: 4","jobGroup":"13","jobId":106,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":2,"numSkippedTasks":55,"numTasks":56,"rowCount":50,"stageIds":[153,151,152],"status":"SUCCEEDED","submissionTime":"2023-04-20T06:49:13.270GMT"},{"completionTime":"2023-04-20T06:49:13.256GMT","dataRead":46620,"dataWritten":4731,"description":"Delta: Job group for statement 13:\n# write the annotations dataframe to a delta lake table\nannotations_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'annotations'))\n\n# write the images dataframe to a delta lake table\nimages_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'images'))\n\n# write the categories dataframe to a delta lake table\ncategories_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'categories')): Compute snapshot for version: 4","jobGroup":"13","jobId":105,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":5,"numTasks":55,"rowCount":120,"stageIds":[150,149],"status":"SUCCEEDED","submissionTime":"2023-04-20T06:49:13.069GMT"},{"completionTime":"2023-04-20T06:49:12.929GMT","dataRead":52031,"dataWritten":46620,"description":"Delta: Job group for statement 13:\n# write the annotations dataframe to a delta lake table\nannotations_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'annotations'))\n\n# write the images dataframe to a delta lake table\nimages_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'images'))\n\n# write the categories dataframe to a delta lake table\ncategories_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'categories')): Compute snapshot for version: 4","jobGroup":"13","jobId":104,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":5,"numCompletedStages":1,"numCompletedTasks":5,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":5,"rowCount":140,"stageIds":[148],"status":"SUCCEEDED","submissionTime":"2023-04-20T06:49:12.596GMT"},{"completionTime":"2023-04-20T06:49:10.830GMT","dataRead":7297,"dataWritten":0,"description":"Job group for statement 13:\n# write the annotations dataframe to a delta lake table\nannotations_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'annotations'))\n\n# write the images dataframe to a delta lake table\nimages_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'images'))\n\n# write the categories dataframe to a delta lake table\ncategories_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'categories'))","jobGroup":"13","jobId":103,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":4,"numTasks":54,"rowCount":28,"stageIds":[147,146],"status":"SUCCEEDED","submissionTime":"2023-04-20T06:49:10.780GMT"},{"completionTime":"2023-04-20T06:49:10.632GMT","dataRead":188741311,"dataWritten":234586217,"description":"Job group for statement 13:\n# write the annotations dataframe to a delta lake table\nannotations_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'annotations'))\n\n# write the images dataframe to a delta lake table\nimages_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'images'))\n\n# write the categories dataframe to a delta lake table\ncategories_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'categories'))","jobGroup":"13","jobId":102,"killedTasksSummary":{},"name":"save at NativeMethodAccessorImpl.java:0","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":49,"numCompletedStages":1,"numCompletedTasks":49,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":49,"rowCount":13510180,"stageIds":[145],"status":"SUCCEEDED","submissionTime":"2023-04-20T06:48:56.583GMT"}],"limit":20,"numbers":{"FAILED":0,"RUNNING":0,"SUCCEEDED":15,"UNKNOWN":0},"rule":"ALL_DESC"},"spark_pool":null,"state":"finished","statement_id":13},"text/plain":["StatementMeta(, 5d1aa8d0-49b9-4260-8dac-2ba686428313, 13, Finished, Available)"]},"metadata":{},"output_type":"display_data"}],"source":["# write the annotations dataframe to a delta lake table\n","annotations_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'annotations'))\n","\n","# write the images dataframe to a delta lake table\n","images_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'images'))\n","\n","# write the categories dataframe to a delta lake table\n","categories_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 'categories'))"]},{"cell_type":"code","execution_count":13,"id":"e028a064-cdac-4833-9f8a-66b89488caff","metadata":{"advisor":{"adviceMetadata":"{\"artifactId\":\"76cad586-c48c-4c6d-b703-019950925c2e\",\"activityId\":\"5d1aa8d0-49b9-4260-8dac-2ba686428313\",\"applicationId\":\"application_1681971504383_0001\",\"jobGroupId\":\"16\",\"advices\":{\"info\":2,\"warn\":2}}"},"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2023-04-20T07:01:38.1236776Z","execution_start_time":"2023-04-20T07:00:42.7239086Z","livy_statement_state":"available","parent_msg_id":"15da15f6-f7fe-4778-bf4d-3eb56e1c0d8b","queued_time":"2023-04-20T07:00:42.4154781Z","session_id":"5d1aa8d0-49b9-4260-8dac-2ba686428313","session_start_time":null,"spark_jobs":{"jobs":[{"completionTime":"2023-04-20T07:01:37.839GMT","dataRead":4400,"dataWritten":0,"description":"Delta: Job group for statement 16:\n# read and write S11 annotations and images\ns11_annotations_df = spark.read.parquet(os.path.join(input_dir, s11_annotations_file_name))\ns11_annotations_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 's11annotations'))\n\ns11_images_df = spark.read.parquet(os.path.join(input_dir, s11_images_file_name))\ns11_images_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 's11images')): Compute snapshot for version: 0","jobGroup":"16","jobId":152,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":2,"numSkippedTasks":51,"numTasks":52,"rowCount":50,"stageIds":[219,217,218],"status":"SUCCEEDED","submissionTime":"2023-04-20T07:01:37.811GMT"},{"completionTime":"2023-04-20T07:01:37.798GMT","dataRead":2136,"dataWritten":4400,"description":"Delta: Job group for statement 16:\n# read and write S11 annotations and images\ns11_annotations_df = spark.read.parquet(os.path.join(input_dir, s11_annotations_file_name))\ns11_annotations_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 's11annotations'))\n\ns11_images_df = spark.read.parquet(os.path.join(input_dir, s11_images_file_name))\ns11_images_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 's11images')): Compute snapshot for version: 0","jobGroup":"16","jobId":151,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":1,"numTasks":51,"rowCount":55,"stageIds":[215,216],"status":"SUCCEEDED","submissionTime":"2023-04-20T07:01:37.635GMT"},{"completionTime":"2023-04-20T07:01:37.500GMT","dataRead":2220,"dataWritten":2136,"description":"Delta: Job group for statement 16:\n# read and write S11 annotations and images\ns11_annotations_df = spark.read.parquet(os.path.join(input_dir, s11_annotations_file_name))\ns11_annotations_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 's11annotations'))\n\ns11_images_df = spark.read.parquet(os.path.join(input_dir, s11_images_file_name))\ns11_images_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 's11images')): Compute snapshot for version: 0","jobGroup":"16","jobId":150,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":1,"rowCount":10,"stageIds":[214],"status":"SUCCEEDED","submissionTime":"2023-04-20T07:01:37.175GMT"},{"completionTime":"2023-04-20T07:01:31.868GMT","dataRead":0,"dataWritten":0,"description":"Job group for statement 16:\n# read and write S11 annotations and images\ns11_annotations_df = spark.read.parquet(os.path.join(input_dir, s11_annotations_file_name))\ns11_annotations_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 's11annotations'))\n\ns11_images_df = spark.read.parquet(os.path.join(input_dir, s11_images_file_name))\ns11_images_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 's11images'))","jobGroup":"16","jobId":149,"killedTasksSummary":{},"name":"","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":0,"numCompletedStages":0,"numCompletedTasks":0,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":0,"rowCount":0,"stageIds":[],"status":"SUCCEEDED","submissionTime":"2023-04-20T07:01:31.868GMT"},{"completionTime":"2023-04-20T07:01:31.751GMT","dataRead":13101793,"dataWritten":13414630,"description":"Job group for statement 16:\n# read and write S11 annotations and images\ns11_annotations_df = spark.read.parquet(os.path.join(input_dir, s11_annotations_file_name))\ns11_annotations_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 's11annotations'))\n\ns11_images_df = spark.read.parquet(os.path.join(input_dir, s11_images_file_name))\ns11_images_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 's11images'))","jobGroup":"16","jobId":148,"killedTasksSummary":{},"name":"save at NativeMethodAccessorImpl.java:0","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":4,"numCompletedStages":1,"numCompletedTasks":4,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":4,"rowCount":998802,"stageIds":[213],"status":"SUCCEEDED","submissionTime":"2023-04-20T07:01:25.189GMT"},{"completionTime":"2023-04-20T07:01:24.213GMT","dataRead":0,"dataWritten":0,"description":"Job group for statement 16:\n# read and write S11 annotations and images\ns11_annotations_df = spark.read.parquet(os.path.join(input_dir, s11_annotations_file_name))\ns11_annotations_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 's11annotations'))\n\ns11_images_df = spark.read.parquet(os.path.join(input_dir, s11_images_file_name))\ns11_images_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 's11images'))","jobGroup":"16","jobId":147,"killedTasksSummary":{},"name":"parquet at <unknown>:0","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":1,"rowCount":0,"stageIds":[212],"status":"SUCCEEDED","submissionTime":"2023-04-20T07:01:23.750GMT"},{"completionTime":"2023-04-20T07:01:23.232GMT","dataRead":4502,"dataWritten":0,"description":"Delta: Job group for statement 16:\n# read and write S11 annotations and images\ns11_annotations_df = spark.read.parquet(os.path.join(input_dir, s11_annotations_file_name))\ns11_annotations_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 's11annotations'))\n\ns11_images_df = spark.read.parquet(os.path.join(input_dir, s11_images_file_name))\ns11_images_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 's11images')): Compute snapshot for version: 0","jobGroup":"16","jobId":146,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":2,"numSkippedTasks":51,"numTasks":52,"rowCount":50,"stageIds":[209,210,211],"status":"SUCCEEDED","submissionTime":"2023-04-20T07:01:23.203GMT"},{"completionTime":"2023-04-20T07:01:23.188GMT","dataRead":2546,"dataWritten":4502,"description":"Delta: Job group for statement 16:\n# read and write S11 annotations and images\ns11_annotations_df = spark.read.parquet(os.path.join(input_dir, s11_annotations_file_name))\ns11_annotations_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 's11annotations'))\n\ns11_images_df = spark.read.parquet(os.path.join(input_dir, s11_images_file_name))\ns11_images_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 's11images')): Compute snapshot for version: 0","jobGroup":"16","jobId":145,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":1,"numTasks":51,"rowCount":55,"stageIds":[208,207],"status":"SUCCEEDED","submissionTime":"2023-04-20T07:01:23.022GMT"},{"completionTime":"2023-04-20T07:01:22.900GMT","dataRead":3440,"dataWritten":2546,"description":"Delta: Job group for statement 16:\n# read and write S11 annotations and images\ns11_annotations_df = spark.read.parquet(os.path.join(input_dir, s11_annotations_file_name))\ns11_annotations_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 's11annotations'))\n\ns11_images_df = spark.read.parquet(os.path.join(input_dir, s11_images_file_name))\ns11_images_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 's11images')): Compute snapshot for version: 0","jobGroup":"16","jobId":144,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":1,"rowCount":10,"stageIds":[206],"status":"SUCCEEDED","submissionTime":"2023-04-20T07:01:22.556GMT"},{"completionTime":"2023-04-20T07:01:20.702GMT","dataRead":0,"dataWritten":0,"description":"Job group for statement 16:\n# read and write S11 annotations and images\ns11_annotations_df = spark.read.parquet(os.path.join(input_dir, s11_annotations_file_name))\ns11_annotations_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 's11annotations'))\n\ns11_images_df = spark.read.parquet(os.path.join(input_dir, s11_images_file_name))\ns11_images_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 's11images'))","jobGroup":"16","jobId":143,"killedTasksSummary":{},"name":"","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":0,"numCompletedStages":0,"numCompletedTasks":0,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":0,"rowCount":0,"stageIds":[],"status":"SUCCEEDED","submissionTime":"2023-04-20T07:01:20.702GMT"},{"completionTime":"2023-04-20T07:01:20.624GMT","dataRead":14238314,"dataWritten":17391704,"description":"Job group for statement 16:\n# read and write S11 annotations and images\ns11_annotations_df = spark.read.parquet(os.path.join(input_dir, s11_annotations_file_name))\ns11_annotations_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 's11annotations'))\n\ns11_images_df = spark.read.parquet(os.path.join(input_dir, s11_images_file_name))\ns11_images_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 's11images'))","jobGroup":"16","jobId":142,"killedTasksSummary":{},"name":"save at NativeMethodAccessorImpl.java:0","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":4,"numCompletedStages":1,"numCompletedTasks":4,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":4,"rowCount":1012910,"stageIds":[205],"status":"SUCCEEDED","submissionTime":"2023-04-20T07:01:11.402GMT"},{"completionTime":"2023-04-20T07:01:10.472GMT","dataRead":0,"dataWritten":0,"description":"Job group for statement 16:\n# read and write S11 annotations and images\ns11_annotations_df = spark.read.parquet(os.path.join(input_dir, s11_annotations_file_name))\ns11_annotations_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 's11annotations'))\n\ns11_images_df = spark.read.parquet(os.path.join(input_dir, s11_images_file_name))\ns11_images_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 's11images'))","jobGroup":"16","jobId":141,"killedTasksSummary":{},"name":"parquet at <unknown>:0","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":1,"rowCount":0,"stageIds":[204],"status":"SUCCEEDED","submissionTime":"2023-04-20T07:01:09.222GMT"}],"limit":20,"numbers":{"FAILED":0,"RUNNING":0,"SUCCEEDED":12,"UNKNOWN":0},"rule":"ALL_DESC"},"spark_pool":null,"state":"finished","statement_id":16},"text/plain":["StatementMeta(, 5d1aa8d0-49b9-4260-8dac-2ba686428313, 16, Finished, Available)"]},"metadata":{},"output_type":"display_data"}],"source":["# read and write S11 annotations and images\n","s11_annotations_df = spark.read.parquet(os.path.join(input_dir, s11_annotations_file_name))\n","s11_annotations_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 's11annotations'))\n","\n","s11_images_df = spark.read.parquet(os.path.join(input_dir, s11_images_file_name))\n","s11_images_df.write.format('delta').mode('overwrite').save(os.path.join(output_dir, 's11images'))"]},{"cell_type":"code","execution_count":null,"id":"4be1127a-0b7b-4dcf-855d-e2f4fcde98e7","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["import json\n","import os\n","import pandas as pd\n","\n","# Set the path to the directory containing the raw JSON data\n","raw_data_path = '/lakehouse/default/Files/raw-data'\n","\n","# Get the list of JSON files in the raw data path, and select the first 10 for the training set\n","train_set = os.listdir(raw_data_path)[:10]\n","\n","# Select the 11th file for the test set\n","test_set = os.listdir(raw_data_path)[10]\n","\n","# Initialize empty DataFrames to store images, annotations, and categories data\n","images = pd.DataFrame()\n","annotations = pd.DataFrame()\n","categories = pd.DataFrame()\n","\n","# Process each JSON file in the training set\n","for file in train_set:\n","    # Read the JSON file and load its data\n","    path = os.path.join(raw_data_path, file)\n","    with open(path) as f:\n","        data = json.load(f)\n","\n","    # Extract and concatenate the 'images' and 'annotations' data into their respective DataFrames\n","    images = pd.concat([images, pd.DataFrame(data['images'])])\n","    annotations = pd.concat([annotations, pd.DataFrame(data['annotations'])])\n","\n","    # The 'categories' data is the same for all files, so we only need to do it once\n","    if len(categories) == 0:\n","        categories = pd.DataFrame(data['categories'])\n","\n","\n","# Process the test set, similar to the training set\n","path = os.path.join(raw_data_path, test_set)\n","with open(path) as f:\n","    data = json.load(f)\n","\n","# Define the paths for saving the data as Delta lake tables\n","train_annotations_table = 'Tables/train_annotations'\n","train_images_table = 'Tables/train_images'\n","test_annotations_table = 'Tables/test_annotations'\n","test_images_table = 'Tables/test_images'\n","categories_table = 'Tables/categories'\n","\n","# Extract and convert the 'images' and 'annotations' data of the test set into DataFrames\n","test_images = pd.DataFrame(data['images'])\n","test_annotations = pd.DataFrame(data['annotations'])\n","\n","# Save the DataFrames as Delta lake tables\n","spark.createDataFrame(images).write.format('delta').mode('overwrite').save(train_images_table)\n","spark.createDataFrame(annotations).write.format('delta').mode('overwrite').save(train_annotations_table)\n","spark.createDataFrame(test_images).write.format('delta').mode('overwrite').save(test_images_table)\n","spark.createDataFrame(test_annotations).write.format('delta').mode('overwrite').save(test_annotations_table)\n","spark.createDataFrame(categories).write.format('delta').mode('overwrite').save(categories_table)\n","\n"]}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"Synapse PySpark","name":"synapse_pyspark"},"language_info":{"name":"python"},"notebook_environment":{},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{},"enableDebugMode":false}},"synapse_widget":{"state":{},"version":"0.1"},"trident":{"lakehouse":{"default_lakehouse":"5f58e739-d87b-404a-9a7b-5e64738a82c5","default_lakehouse_name":"Serengeti_Lakehouse","default_lakehouse_workspace_id":"1eaa972c-c94a-47fa-90f5-bcad0a19f945","known_lakehouses":[{"id":"5f58e739-d87b-404a-9a7b-5e64738a82c5"}]}},"widgets":{}},"nbformat":4,"nbformat_minor":5}
