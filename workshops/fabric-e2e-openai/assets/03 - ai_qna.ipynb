{"cells":[{"cell_type":"markdown","source":["# A Guide to Q&A on PDF Documents"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"41f9aaa6-7dd6-4389-baf8-43d9fe2280f6"},{"cell_type":"markdown","source":["## Introduction\n","In this notebook, we'll demonstrate how to develop a context-aware question answering framework for any form of a document using [OpenAI models](https://azure.microsoft.com/products/ai-services/openai-service), [SynapseML](https://microsoft.github.io/SynapseML/) and [Azure AI Services](https://azure.microsoft.com/products/cognitive-services/). In this notebook, we assume that PDF documents are the source of data, however, the same framework can be easiy extended to other document formats too.   \n","\n","We’ll cover the following key steps:\n","\n","1. Preprocessing PDF Documents: Learn how to load the PDF documents into a Spark DataFrame, read the documents using the [Azure AI Document Intelligence](https://azure.microsoft.com/products/ai-services/ai-document-intelligence) in Azure AI Services, and use SynapseML to split the documents into chunks.\n","2. Embedding Generation and Storage: Learn how to generate embeddings for the chunks using SynapseML and [Azure OpenAI Services](https://azure.microsoft.com/products/cognitive-services/openai-service), store the embeddings in a vector store using [Azure Cognitive Search](https://azure.microsoft.com/products/search), and search the vector store to answer the user’s question.\n","3. Question Answering Pipeline: Learn how to retrieve relevant document based on the user’s question and provide the answer using [Langchain](https://python.langchain.com/en/latest/index.html#)."],"metadata":{},"id":"30f24482-5513-4050-a382-6795d5613a15"},{"cell_type":"markdown","source":["We start by installing the necessary python libraries."],"metadata":{},"id":"7bcced7a-a83f-465b-9ab9-83e7c9067915"},{"cell_type":"code","source":["%pip install langchain openai semantic-link"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":null,"statement_id":-1,"state":"finished","livy_statement_state":"available","queued_time":"2023-10-27T10:53:54.9774799Z","session_start_time":null,"execution_start_time":"2023-10-27T10:54:29.185035Z","execution_finish_time":"2023-10-27T10:54:29.185179Z","spark_jobs":null,"parent_msg_id":"224f7b9a-7f51-499e-8de5-8caf82ebad4f"},"text/plain":"StatementMeta(, , -1, Finished, Available)"},"metadata":{}},{"output_type":"execute_result","execution_count":1,"data":{},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Collecting langchain\n  Downloading langchain-0.0.324-py3-none-any.whl (1.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m79.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting openai\n  Downloading openai-0.28.1-py3-none-any.whl (76 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting semantic-link\n  Downloading semantic_link-0.3.5-py3-none-any.whl (8.2 kB)\nCollecting jsonpatch<2.0,>=1.33\n  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\nRequirement already satisfied: numpy<2,>=1 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from langchain) (1.23.5)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from langchain) (2.0.9)\nCollecting dataclasses-json<0.7,>=0.5.7\n  Downloading dataclasses_json-0.6.1-py3-none-any.whl (27 kB)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from langchain) (3.8.4)\nCollecting langsmith<0.1.0,>=0.0.52\n  Downloading langsmith-0.0.52-py3-none-any.whl (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.3/43.3 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting pydantic<3,>=1\n  Downloading pydantic-2.4.2-py3-none-any.whl (395 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m395.8/395.8 kB\u001b[0m \u001b[31m118.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from langchain) (4.0.2)\nRequirement already satisfied: PyYAML>=5.3 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from langchain) (6.0)\nRequirement already satisfied: anyio<4.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from langchain) (3.6.2)\nRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from langchain) (8.2.2)\nRequirement already satisfied: requests<3,>=2 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from langchain) (2.28.2)\nRequirement already satisfied: tqdm in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from openai) (4.65.0)\nCollecting semantic-link-functions-validators==0.3.5\n  Downloading semantic_link_functions_validators-0.3.5-py3-none-any.whl (4.7 kB)\nCollecting semantic-link-functions-phonenumbers==0.3.5\n  Downloading semantic_link_functions_phonenumbers-0.3.5-py3-none-any.whl (4.3 kB)\nCollecting semantic-link-functions-geopandas==0.3.5\n  Downloading semantic_link_functions_geopandas-0.3.5-py3-none-any.whl (4.0 kB)\nCollecting semantic-link-functions-holidays==0.3.5\n  Downloading semantic_link_functions_holidays-0.3.5-py3-none-any.whl (4.2 kB)\nCollecting semantic-link-functions-meteostat==0.3.5\n  Downloading semantic_link_functions_meteostat-0.3.5-py3-none-any.whl (4.5 kB)\nCollecting semantic-link-sempy==0.3.5\n  Downloading semantic_link_sempy-0.3.5-py3-none-any.whl (2.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m158.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting folium\n  Downloading folium-0.14.0-py2.py3-none-any.whl (102 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.3/102.3 kB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting mapclassify\n  Downloading mapclassify-2.6.1-py3-none-any.whl (38 kB)\nCollecting geopandas\n  Downloading geopandas-0.14.0-py3-none-any.whl (1.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m164.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting holidays\n  Downloading holidays-0.35-py3-none-any.whl (800 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m801.0/801.0 kB\u001b[0m \u001b[31m166.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting meteostat\n  Downloading meteostat-1.6.7-py3-none-any.whl (31 kB)\nCollecting phonenumbers\n  Downloading phonenumbers-8.13.23-py2.py3-none-any.whl (2.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m184.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting validators\n  Downloading validators-0.22.0-py3-none-any.whl (26 kB)\nCollecting pythonnet==3.0.1\n  Downloading pythonnet-3.0.1-py3-none-any.whl (284 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.5/284.5 kB\u001b[0m \u001b[31m105.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting clr-loader<0.3.0,>=0.2.2\n  Downloading clr_loader-0.2.6-py3-none-any.whl (51 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.3/51.3 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: yarl<2.0,>=1.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.8.2)\nRequirement already satisfied: multidict<7.0,>=4.5 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\nRequirement already satisfied: frozenlist>=1.1.1 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\nRequirement already satisfied: attrs>=17.3.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (22.2.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.1.1)\nRequirement already satisfied: idna>=2.8 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from anyio<4.0->langchain) (3.4)\nRequirement already satisfied: sniffio>=1.1 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from anyio<4.0->langchain) (1.3.0)\nCollecting marshmallow<4.0.0,>=3.18.0\n  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting typing-inspect<1,>=0.4.0\n  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\nCollecting jsonpointer>=1.9\n  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\nCollecting annotated-types>=0.4.0\n  Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\nCollecting pydantic-core==2.10.1\n  Downloading pydantic_core-2.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m190.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting typing-extensions>=4.6.1\n  Using cached typing_extensions-4.8.0-py3-none-any.whl (31 kB)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from requests<3,>=2->langchain) (1.26.14)\nRequirement already satisfied: certifi>=2017.4.17 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2022.12.7)\nRequirement already satisfied: greenlet!=0.4.17 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\nRequirement already satisfied: packaging>=17.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (22.0)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (0.4.4)\nRequirement already satisfied: jinja2>=2.9 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from folium->semantic-link-functions-geopandas==0.3.5->semantic-link) (3.1.2)\nCollecting branca>=0.6.0\n  Downloading branca-0.6.0-py3-none-any.whl (24 kB)\nCollecting fiona>=1.8.21\n  Downloading fiona-1.9.5-cp310-cp310-manylinux2014_x86_64.whl (15.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting pyproj>=3.3.0\n  Downloading pyproj-3.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m188.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting shapely>=1.8.0\n  Downloading shapely-2.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m203.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pandas>=1.4.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from geopandas->semantic-link-functions-geopandas==0.3.5->semantic-link) (1.5.3)\nRequirement already satisfied: python-dateutil in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from holidays->semantic-link-functions-holidays==0.3.5->semantic-link) (2.8.2)\nRequirement already satisfied: scikit-learn>=1.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from mapclassify->semantic-link-functions-geopandas==0.3.5->semantic-link) (1.2.0)\nRequirement already satisfied: scipy>=1.8 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from mapclassify->semantic-link-functions-geopandas==0.3.5->semantic-link) (1.10.1)\nCollecting networkx>=2.7\n  Downloading networkx-3.2-py3-none-any.whl (1.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m183.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pytz in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from meteostat->semantic-link-functions-meteostat==0.3.5->semantic-link) (2022.7.1)\nRequirement already satisfied: cffi>=1.13 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from clr-loader<0.3.0,>=0.2.2->pythonnet==3.0.1->semantic-link-sempy==0.3.5->semantic-link) (1.15.1)\nCollecting cligj>=0.5\n  Downloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\nRequirement already satisfied: setuptools in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from fiona>=1.8.21->geopandas->semantic-link-functions-geopandas==0.3.5->semantic-link) (67.6.1)\nCollecting click-plugins>=1.0\n  Downloading click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\nRequirement already satisfied: click~=8.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from fiona>=1.8.21->geopandas->semantic-link-functions-geopandas==0.3.5->semantic-link) (8.1.3)\nRequirement already satisfied: six in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from fiona>=1.8.21->geopandas->semantic-link-functions-geopandas==0.3.5->semantic-link) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from jinja2>=2.9->folium->semantic-link-functions-geopandas==0.3.5->semantic-link) (2.1.2)\nRequirement already satisfied: joblib>=1.1.1 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from scikit-learn>=1.0->mapclassify->semantic-link-functions-geopandas==0.3.5->semantic-link) (1.2.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from scikit-learn>=1.0->mapclassify->semantic-link-functions-geopandas==0.3.5->semantic-link) (3.1.0)\nRequirement already satisfied: pycparser in /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages (from cffi>=1.13->clr-loader<0.3.0,>=0.2.2->pythonnet==3.0.1->semantic-link-sempy==0.3.5->semantic-link) (2.21)\nInstalling collected packages: phonenumbers, validators, typing-extensions, shapely, pyproj, networkx, marshmallow, jsonpointer, cligj, click-plugins, annotated-types, typing-inspect, pydantic-core, jsonpatch, holidays, fiona, clr-loader, branca, pythonnet, pydantic, openai, meteostat, mapclassify, geopandas, folium, dataclasses-json, semantic-link-sempy, langsmith, semantic-link-functions-validators, semantic-link-functions-phonenumbers, semantic-link-functions-meteostat, semantic-link-functions-holidays, semantic-link-functions-geopandas, langchain, semantic-link\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing_extensions 4.5.0\n    Not uninstalling typing-extensions at /home/trusted-service-user/cluster-env/trident_env/lib/python3.10/site-packages, outside environment /nfs4/pyenv-70810b7e-d24e-4aac-bdb3-b0aa0e8b17fb\n    Can't uninstall 'typing_extensions'. No files were found to uninstall.\nSuccessfully installed annotated-types-0.6.0 branca-0.6.0 click-plugins-1.1.1 cligj-0.7.2 clr-loader-0.2.6 dataclasses-json-0.6.1 fiona-1.9.5 folium-0.14.0 geopandas-0.14.0 holidays-0.35 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.324 langsmith-0.0.52 mapclassify-2.6.1 marshmallow-3.20.1 meteostat-1.6.7 networkx-3.2 openai-0.28.1 phonenumbers-8.13.23 pydantic-2.4.2 pydantic-core-2.10.1 pyproj-3.6.1 pythonnet-3.0.1 semantic-link-0.3.5 semantic-link-functions-geopandas-0.3.5 semantic-link-functions-holidays-0.3.5 semantic-link-functions-meteostat-0.3.5 semantic-link-functions-phonenumbers-0.3.5 semantic-link-functions-validators-0.3.5 semantic-link-sempy-0.3.5 shapely-2.0.2 typing-extensions-4.8.0 typing-inspect-0.9.0 validators-0.22.0\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/nfs4/pyenv-70810b7e-d24e-4aac-bdb3-b0aa0e8b17fb/bin/python -m pip install --upgrade pip\u001b[0m\nNote: you may need to restart the kernel to use updated packages.\n"]},{"output_type":"execute_result","execution_count":1,"data":{},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Warning: PySpark kernel has been restarted to use updated packages.\n\n"]}],"execution_count":1,"metadata":{},"id":"2fbf0e3d-b2ba-4500-9c24-4f86e49aa10c"},{"cell_type":"markdown","source":["### Step 1: Provide the keys for Azure AI Services and Azure OpenAI to authenticate the applications.\n","\n","To authenticate Azure AI Services and Azure OpenAI applications, you need to provide the respective API keys. Here is an example of how you can provide the keys in Python code. find_secret() function uses Azure Keyvault to get the API keys, however you can directly paste your own keys there."],"metadata":{},"id":"9cca1ab0-7727-4e9a-a484-bdddf56c8d4e"},{"cell_type":"code","source":["# Key Vault\n","key_vault_endpoint = 'https://ai-vault-store.vault.azure.net/'\n","\n","# Azure Open AI\n","aoai_service_name = mssparkutils.credentials.getSecret(key_vault_endpoint, 'aoai-service-name')\n","aoai_endpoint = f'https://{aoai_service_name}.openai.azure.com/'\n","aoai_key = mssparkutils.credentials.getSecret(key_vault_endpoint, 'aoai-key')\n","aoai_deployment_name_embeddings = \"text-embedding-ada-002\"\n","aoai_deployment_name_completions = \"gpt-4\"\n","\n","# Azure Cognitive Search\n","cogsearch_name = mssparkutils.credentials.getSecret(key_vault_endpoint, 'cogsearch-name')\n","cogsearch_index_name = 'wwireports'\n","cogsearch_api_key = mssparkutils.credentials.getSecret(key_vault_endpoint, 'cogsearch-api-key')\n","\n","# Azure AI Service\n","ai_services_key = mssparkutils.credentials.getSecret(key_vault_endpoint, 'ai-services-key')\n","ai_services_location = mssparkutils.credentials.getSecret(key_vault_endpoint, 'ai-services-location')"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"2635f712-c84c-48f2-9827-8010b16ce61f","statement_id":3,"state":"submitted","livy_statement_state":"waiting","queued_time":"2023-10-27T11:25:56.6222718Z","session_start_time":"2023-10-27T11:25:56.9779611Z","execution_start_time":"2023-10-27T11:26:07.3102681Z","execution_finish_time":null,"spark_jobs":null,"parent_msg_id":"82f659ca-badd-4fbe-88c7-a122c9363441"},"text/plain":"StatementMeta(, 2635f712-c84c-48f2-9827-8010b16ce61f, 3, Submitted, Waiting)"},"metadata":{}}],"execution_count":1,"metadata":{},"id":"a3375a1c-2b20-4b2f-9db9-9e1fef7ada27"},{"cell_type":"markdown","source":["### Step 2: Load the PDF documents into a Spark DataFrame.\n","\n","To load PDF documents into a Spark DataFrame use the `spark.read.format(\"binaryFile\")`method provided by Apache Spark.\n","\n","This code will read the PDF documents and create a Spark DataFrame named df with the contents of the PDFs. The DataFrame will have a schema that represents the structure of the PDF documents, including their textual content."],"metadata":{},"id":"40f3d9c9-ca6f-4d11-98f0-f4e93e6091fc"},{"cell_type":"code","source":["# Import required pyspark libraries\n","\n","from pyspark.sql.functions import udf\n","from pyspark.sql.types import StringType\n","\n","document_path = 'Files/wwi-doc-reports/wwi-importers-bi-report.pdf'\n","\n","df = (\n","    spark.read.format('binaryFile')\n","    .load(document_path)\n","    .limit(10)\n","    .cache()\n",")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":null,"statement_id":null,"state":"cancelled","livy_statement_state":null,"queued_time":"2023-10-27T10:53:54.9861257Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":"2023-10-27T10:54:37.3110044Z","spark_jobs":null,"parent_msg_id":"ffa96638-3993-487f-8839-a9033d873e9d"},"text/plain":"StatementMeta(, , , Cancelled, )"},"metadata":{}}],"execution_count":null,"metadata":{},"id":"8b7c7fb3-cc0b-4d2e-aa05-c85896ab8376"},{"cell_type":"markdown","source":["### Step 3: Read the documents using Azure AI Document Intelligence.\n","\n","We utilize [SynapseML](https://microsoft.github.io/SynapseML/), an ecosystem of tools designed to enhance the distributed computing framework [Apache Spark](https://github.com/apache/spark). SynapseML introduces advanced networking capabilities to the Spark ecosystem and offers user-friendly SparkML transformers for various [Azure AI Services](https://azure.microsoft.com/products/ai-services).\n","\n","Additionally, we employ AnalyzeDocument from Azure AI Services to extract the complete document content and present it in the designated columns called \"output_content\" and \"paragraph.\""],"metadata":{},"id":"37dbd046-a574-449d-8247-1f43daadf47a"},{"cell_type":"code","source":["from synapse.ml.cognitive import AnalyzeDocument\n","from pyspark.sql.functions import col\n","\n","analyze_document = (\n","    AnalyzeDocument()\n","    .setPrebuiltModelId(\"prebuilt-layout\")\n","    .setSubscriptionKey(ai_services_key)\n","    .setLocation(ai_services_location)\n","    .setImageBytesCol(\"content\")\n","    .setOutputCol(\"result\")\n",")\n","\n","analyzed_df = (\n","    analyze_document.transform(df)\n","    .withColumn(\"output_content\", col(\"result.analyzeResult.content\"))\n","    .withColumn(\"paragraphs\", col(\"result.analyzeResult.paragraphs\"))\n",").cache()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":null,"statement_id":null,"state":"cancelled","livy_statement_state":null,"queued_time":"2023-10-27T10:53:54.9867212Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":"2023-10-27T10:54:37.3113618Z","spark_jobs":null,"parent_msg_id":"2e13b437-f6a0-4f2a-ab5b-b2a667b072b8"},"text/plain":"StatementMeta(, , , Cancelled, )"},"metadata":{}}],"execution_count":null,"metadata":{},"id":"82b810b1-e8fb-473b-a335-6214eeab0b7b"},{"cell_type":"markdown","source":["We can observe the analayzed Spark DataFrame named ```analyzed_df``` using the following code. Note that we drop the \"content\" column as it is not needed anymore."],"metadata":{},"id":"0a1b0813-b121-4e61-9920-8058535f4ddd"},{"cell_type":"code","source":["analyzed_df = analyzed_df.drop(\"content\")\n","display(analyzed_df)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":null,"statement_id":null,"state":"cancelled","livy_statement_state":null,"queued_time":"2023-10-27T10:53:54.9872796Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":"2023-10-27T10:54:37.311649Z","spark_jobs":null,"parent_msg_id":"cef3674b-9372-42bc-ab39-f2c851dec642"},"text/plain":"StatementMeta(, , , Cancelled, )"},"metadata":{}}],"execution_count":null,"metadata":{"collapsed":false},"id":"85ea568a-1a52-4266-a441-790c7efd295e"},{"cell_type":"markdown","source":["### Step 4: Split the documents into chunks.\n","\n","After analyzing the document, we leverage SynapseML’s PageSplitter to divide the documents into smaller sections, which are subsequently stored in the “chunks” column. This allows for more granular representation and processing of the document content."],"metadata":{},"id":"29136e17-6e1e-4da9-b54d-f0f90c938ac7"},{"cell_type":"code","source":["from synapse.ml.featurize.text import PageSplitter\n","\n","ps = (\n","    PageSplitter()\n","    .setInputCol(\"output_content\")\n","    .setMaximumPageLength(4000)\n","    .setMinimumPageLength(3000)\n","    .setOutputCol(\"chunks\")\n",")\n","\n","splitted_df = ps.transform(analyzed_df)\n","display(splitted_df)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":null,"statement_id":null,"state":"cancelled","livy_statement_state":null,"queued_time":"2023-10-27T10:53:54.9878298Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":"2023-10-27T10:54:37.3122669Z","spark_jobs":null,"parent_msg_id":"57298540-967d-4b5c-ade2-508e4db3f178"},"text/plain":"StatementMeta(, , , Cancelled, )"},"metadata":{}}],"execution_count":null,"metadata":{"collapsed":false},"id":"11b907e4-1f25-406a-b347-9ceba5724055"},{"cell_type":"markdown","source":["Note that the chunks for each document are presented in a single row inside an array. In order to embed all the chunks in the following cells, we need to have each chunk in a separate row. To accomplish that, we first explode these arrays so there is only one chunk in each row, then filter the Spark DataFrame in order to only keep the path to the document and the chunk in a single row."],"metadata":{},"id":"a7b12e0c-eab9-4b0f-8418-9b7529928415"},{"cell_type":"code","source":["# Each column contains many chunks for the saame document as a vector.\n","# Explode will distribute and replicate the content of a vecor across multple rows\n","from pyspark.sql.functions import explode, col\n","\n","exploded_df = splitted_df.select(\"path\", explode(col(\"chunks\")).alias(\"chunk\")).select(\n","    \"path\", \"chunk\"\n",")\n","display(exploded_df)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":null,"statement_id":null,"state":"cancelled","livy_statement_state":null,"queued_time":"2023-10-27T10:53:54.9883003Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":"2023-10-27T10:54:37.3124998Z","spark_jobs":null,"parent_msg_id":"843cce49-321f-4667-8a31-9e9b31376cad"},"text/plain":"StatementMeta(, , , Cancelled, )"},"metadata":{}}],"execution_count":null,"metadata":{"collapsed":false},"id":"f2b10b83-b47a-4a09-b59a-812b34632211"},{"cell_type":"markdown","source":["### Step 5: Generate Embeddings.\n","\n","To produce embeddings for each chunk, we utilize both SynapseML and Azure OpenAI Service. By integrating the Azure OpenAI service with SynapseML, we can leverage the power of the Apache Spark distributed computing framework to process numerous prompts using the OpenAI service. This integration enables the SynapseML embedding client to generate embeddings in a distributed manner, enabling efficient processing of large volumes of data. If you're interested in applying large language models at a distributed scale using Azure OpenAI and Azure Synapse Analytics, you can refer to [this approach](https://microsoft.github.io/SynapseML/docs/Explore%20Algorithms/OpenAI/). For more detailed information on generating embeddings with Azure OpenAI, you can look [here]( https://learn.microsoft.com/azure/cognitive-services/openai/how-to/embeddings?tabs=console).\n"],"metadata":{},"id":"e9a98f0c-b3a3-43f3-97c2-fc8955268e4e"},{"cell_type":"code","source":["from synapse.ml.cognitive import OpenAIEmbedding\n","\n","embedding = (\n","    OpenAIEmbedding()\n","    .setSubscriptionKey(aoai_key)\n","    .setDeploymentName(aoai_deployment_name_embeddings)\n","    .setCustomServiceName(aoai_service_name)\n","    .setTextCol(\"chunk\")\n","    .setErrorCol(\"error\")\n","    .setOutputCol(\"embeddings\")\n",")\n","\n","df_embeddings = embedding.transform(exploded_df)\n","\n","display(df_embeddings)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":null,"statement_id":null,"state":"cancelled","livy_statement_state":null,"queued_time":"2023-10-27T10:53:54.9887812Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":"2023-10-27T10:54:37.3126613Z","spark_jobs":null,"parent_msg_id":"f64537e0-81f1-4a0c-a1c0-13be622a3d57"},"text/plain":"StatementMeta(, , , Cancelled, )"},"metadata":{}}],"execution_count":null,"metadata":{"collapsed":false},"id":"2eb9349d-6b5a-4672-bfd6-7022b76e3e8b"},{"cell_type":"markdown","source":["### Step 6: Store the embeddings in Azure Cognitive Search Vector Store.\n","\n","[Azure Cognitive Search](https://learn.microsoft.com/azure/search/search-what-is-azure-search) offers a user-friendly interface for creating a vector database, as well as storing and retrieving data using vector search. If you're interested in learning more about vector search, you can look [here](https://github.com/Azure/cognitive-search-vector-pr/tree/main).\n","\n","\n","Storing data in the AzureCogSearch vector database involves two main steps:\n","\n","Creating the Index: The first step is to establish the index or schema of the vector database. This entails defining the structure and properties of the data that will be stored and indexed in the vector database.\n","\n","Adding Chunked Documents and Embeddings: The second step involves adding the chunked documents, along with their corresponding embeddings, to the vector datastore. This allows for efficient storage and retrieval of the data using vector search capabilities.\n","\n","By following these steps, you can effectively store your chunked documents and their associated embeddings in the AzureCogSearch vector database, enabling seamless retrieval of relevant information through vector search functionality."],"metadata":{},"id":"0c5618df-a0fc-416a-9308-d0f931407f94"},{"cell_type":"code","source":["# Import necessary packages\n","import requests\n","import json\n","\n","EMBEDDING_LENGTH = (\n","    1536  # length of the embedding vector (OpenAI generates embeddings of length 1536)\n",")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":null,"statement_id":null,"state":"cancelled","livy_statement_state":null,"queued_time":"2023-10-27T10:53:54.9893031Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":"2023-10-27T10:54:37.3128796Z","spark_jobs":null,"parent_msg_id":"7eb7a97c-a609-4f36-857b-8a91ac3367fb"},"text/plain":"StatementMeta(, , , Cancelled, )"},"metadata":{}}],"execution_count":null,"metadata":{},"id":"3947fdae-41b7-43be-b67c-418e092462bc"},{"cell_type":"code","source":["# Create Index for Cog Search with fields as id, content, and contentVector\n","# Note the datatypes for each field below\n","\n","url = f\"https://{cogsearch_name}.search.windows.net/indexes/{cogsearch_index_name}?api-version=2023-07-01-Preview\"\n","payload = json.dumps(\n","    {\n","        \"name\": cogsearch_index_name,\n","        \"fields\": [\n","            {\"name\": \"id\", \"type\": \"Edm.String\", \"key\": True, \"filterable\": True},\n","            {\n","                \"name\": \"content\",\n","                \"type\": \"Edm.String\",\n","                \"searchable\": True,\n","                \"retrievable\": True,\n","            },\n","            {\n","                \"name\": \"contentVector\",\n","                \"type\": \"Collection(Edm.Single)\",\n","                \"searchable\": True,\n","                \"retrievable\": True,\n","                \"dimensions\": EMBEDDING_LENGTH,\n","                \"vectorSearchConfiguration\": \"vectorConfig\",\n","            },\n","        ],\n","        \"vectorSearch\": {\n","            \"algorithmConfigurations\": [\n","                {\n","                    \"name\": \"vectorConfig\",\n","                    \"kind\": \"hnsw\",\n","                }\n","            ]\n","        },\n","    }\n",")\n","headers = {\"Content-Type\": \"application/json\", \"api-key\": cogsearch_api_key}\n","\n","response = requests.request(\"PUT\", url, headers=headers, data=payload)\n","print(response.status_code)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":null,"statement_id":null,"state":"cancelled","livy_statement_state":null,"queued_time":"2023-10-27T10:53:54.9898214Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":"2023-10-27T10:54:37.3130948Z","spark_jobs":null,"parent_msg_id":"7148107b-183a-4582-b0fa-3d664a6e5bb4"},"text/plain":"StatementMeta(, , , Cancelled, )"},"metadata":{}}],"execution_count":null,"metadata":{},"id":"116486de-7298-44f3-8166-ab1682793ab8"},{"cell_type":"markdown","source":["We need to use User Defined Function (UDF) through the udf() method in order to apply functions directly to the DataFrames and SQL databases in Python, without any need to individually register them."],"metadata":{},"id":"cea7027c-68e1-4453-8975-bfb30ed5c4fe"},{"cell_type":"code","source":["# Use Spark's UDF to insert entries to Cognitive Search\n","# This allows to run the code in a distributed fashion\n","\n","# Define a UDF using the @udf decorator\n","@udf(returnType=StringType())\n","def insert_to_cog_search(idx, content, contentVector):\n","    url = f\"https://{cogsearch_name}.search.windows.net/indexes/{cogsearch_index_name}/docs/index?api-version=2023-07-01-Preview\"\n","\n","    payload = json.dumps(\n","        {\n","            \"value\": [\n","                {\n","                    \"id\": str(idx),\n","                    \"content\": content,\n","                    \"contentVector\": contentVector.tolist(),\n","                    \"@search.action\": \"upload\",\n","                },\n","            ]\n","        }\n","    )\n","    headers = {\n","        \"Content-Type\": \"application/json\",\n","        \"api-key\": cogsearch_api_key,\n","    }\n","\n","    response = requests.request(\"POST\", url, headers=headers, data=payload)\n","    # response.text\n","\n","    if response.status_code == 200 or response.status_code == 201:\n","        return \"Success\"\n","    else:\n","        return \"Failure\""],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":null,"statement_id":null,"state":"cancelled","livy_statement_state":null,"queued_time":"2023-10-27T10:53:54.9903575Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":"2023-10-27T10:54:37.313255Z","spark_jobs":null,"parent_msg_id":"004664d4-9a4c-4fcb-be54-126e53edc37f"},"text/plain":"StatementMeta(, , , Cancelled, )"},"metadata":{}}],"execution_count":null,"metadata":{},"id":"806f8eec-2bc8-438a-9225-4fc685b6978b"},{"cell_type":"markdown","source":["In the following, we apply UDF to different columns. Note that UDF also helps to add new columns to the DataFrame."],"metadata":{},"id":"f7186953-2143-4a44-bd6c-8c4126729d72"},{"cell_type":"code","source":["# Apply the UDF on the different columns\n","from pyspark.sql.functions import monotonically_increasing_id\n","\n","df_embeddings = df_embeddings.withColumn(\n","    \"idx\", monotonically_increasing_id()\n",")  ## adding a column with id\n","df_embeddings = df_embeddings.withColumn(\n","    \"errorCogSearch\",\n","    insert_to_cog_search(\n","        df_embeddings[\"idx\"], df_embeddings[\"chunk\"], df_embeddings[\"embeddings\"]\n","    ),\n",")\n","\n","# Show the transformed DataFrame\n","df_embeddings.show()\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":null,"statement_id":null,"state":"cancelled","livy_statement_state":null,"queued_time":"2023-10-27T10:53:54.9909323Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":"2023-10-27T10:54:37.3134034Z","spark_jobs":null,"parent_msg_id":"7dc5545d-efe4-4f1b-b471-181ab713d0ab"},"text/plain":"StatementMeta(, , , Cancelled, )"},"metadata":{}}],"execution_count":null,"metadata":{},"id":"45ecc9de-9546-4c66-9ce6-7c9eba8f7ac1"},{"cell_type":"markdown","source":["### Step 7: Ask a Question.\n","\n","After processing the document, we can proceed to pose a question. We will use [SynapseML](https://microsoft.github.io/SynapseML/docs/Explore%20Algorithms/OpenAI/Quickstart%20-%20OpenAI%20Embedding/) to convert the user's question into an embedding and then utilize cosine similarity to retrieve the top K document chunks that closely match the user's question. It's worth mentioning that alternative similarity metrics can also be employed."],"metadata":{},"id":"306a6b0b-5d8a-494c-8a5f-0887448efe25"},{"cell_type":"code","source":["# Ask a question and convert to embeddings\n","\n","\n","def gen_question_embedding(user_question):\n","    # Convert question to embedding using synapseML\n","    from synapse.ml.cognitive import OpenAIEmbedding\n","\n","    df_ques = spark.createDataFrame([(user_question, 1)], [\"questions\", \"dummy\"])\n","    embedding = (\n","        OpenAIEmbedding()\n","        .setSubscriptionKey(aoai_key)\n","        .setDeploymentName(aoai_deployment_name_embeddings)\n","        .setCustomServiceName(aoai_service_name)\n","        .setTextCol(\"questions\")\n","        .setErrorCol(\"errorQ\")\n","        .setOutputCol(\"embeddings\")\n","    )\n","    df_ques_embeddings = embedding.transform(df_ques)\n","    row = df_ques_embeddings.collect()[0]\n","    question_embedding = row.embeddings.tolist()\n","    return question_embedding\n","\n","\n","def retrieve_k_chunk(k, question_embedding):\n","    # Retrieve the top K entries\n","    url = f\"https://{cogsearch_name}.search.windows.net/indexes/{cogsearch_index_name}/docs/search?api-version=2023-07-01-Preview\"\n","\n","    payload = json.dumps(\n","        {\"vector\": {\"value\": question_embedding, \"fields\": \"contentVector\", \"k\": 2}}\n","    )\n","    headers = {\n","        \"Content-Type\": \"application/json\",\n","        \"api-key\": cogsearch_api_key,\n","    }\n","\n","    response = requests.request(\"POST\", url, headers=headers, data=payload)\n","    output = json.loads(response.text)\n","    print(response.status_code)\n","    return output"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":null,"statement_id":null,"state":"cancelled","livy_statement_state":null,"queued_time":"2023-10-27T10:53:54.9915105Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":"2023-10-27T10:54:37.313553Z","spark_jobs":null,"parent_msg_id":"312a5fde-b0c9-45ea-883c-f3e3ee4daee0"},"text/plain":"StatementMeta(, , , Cancelled, )"},"metadata":{}}],"execution_count":null,"metadata":{},"id":"2b237806-d4e3-4cf2-aef4-f5a0506a01d3"},{"cell_type":"markdown","source":["### Step 8: Respond to a User’s Question.\n","\n","To provide a response to the user's question, we will utilize the [LangChain](https://python.langchain.com/en/latest/index.html) framework. With the LangChain framework we will augment the retrieved documents with respect to the user's question. Following this, we can request a response to the user's question from our framework."],"metadata":{},"id":"b220c332-cf22-478a-8332-67de974cf2f5"},{"cell_type":"code","source":["# Import necenssary libraries and setting up OpenAI\n","from langchain.chat_models import AzureChatOpenAI\n","from langchain import PromptTemplate\n","from langchain.chains import LLMChain\n","import openai\n","\n","openai.api_type = \"azure\"\n","openai.api_base = aoai_endpoint\n","openai.api_version = \"2022-12-01\"\n","openai.api_key = aoai_key"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":null,"statement_id":null,"state":"cancelled","livy_statement_state":null,"queued_time":"2023-10-27T10:53:54.9920982Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":"2023-10-27T10:54:37.3137467Z","spark_jobs":null,"parent_msg_id":"9e116afd-6aa6-487f-aeaa-7bb3b504f34a"},"text/plain":"StatementMeta(, , , Cancelled, )"},"metadata":{}}],"execution_count":null,"metadata":{},"id":"6bb11d85-d9c5-47ac-afe0-40eb92823a1e"},{"cell_type":"markdown","source":["We can now wrap up the Q&A journey by asking a question and checking the answer. "],"metadata":{},"id":"4d88dddd-c194-4750-9fde-9a760d2a19d2"},{"cell_type":"code","source":["# Define a Question Answering chain function using LangChain\n","def qa_chain_func():\n","\n","    # Define llm model\n","    llm = AzureChatOpenAI(\n","    openai_api_base=aoai_endpoint,\n","    openai_api_version=\"2023-05-15\",\n","    deployment_name=aoai_deployment_name_completions,\n","    openai_api_key=aoai_key,\n","    openai_api_type=\"azure\",\n","    temperature=0\n",")\n","\n","    # Write a preprompt with context and query as variables \n","    template = \"\"\"\n","    context :{context}\n","    Answer the question based on the context above. If the\n","    information to answer the question is not present in the given context then reply \"I don't know\".\n","    Question: {query}\n","    Answer: \"\"\"\n","\n","    # Define a prompt template\n","    prompt_template = PromptTemplate(\n","        input_variables=[\"context\", \"query\"], template=template\n","    )\n","    # Define a chain\n","    qa_chain = LLMChain(llm=llm, prompt=prompt_template)\n","    return qa_chain"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":null,"statement_id":null,"state":"cancelled","livy_statement_state":null,"queued_time":"2023-10-27T10:53:54.9927248Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":"2023-10-27T10:54:37.3139028Z","spark_jobs":null,"parent_msg_id":"c7ae286a-64d8-46a8-8221-77f519053c34"},"text/plain":"StatementMeta(, , , Cancelled, )"},"metadata":{}}],"execution_count":null,"metadata":{},"id":"b996d1da-55dd-478f-b379-e337c1660ce4"},{"cell_type":"code","source":["def get_answer(user_question):\n","    retrieve_k = 5  # Retrieve the top 5 documents from vector database\n","\n","    # Generate embeddings for the question and retrieve the top k document chunks\n","    question_embedding = gen_question_embedding(user_question)\n","    output = retrieve_k_chunk(retrieve_k, question_embedding)\n","\n","    # Concatenate the content of retrieved documents\n","    context = [i[\"content\"] for i in output[\"value\"]]\n","\n","    # Make a Quesion Answer chain function and pass\n","    qa_chain = qa_chain_func()\n","    answer = qa_chain.run({\"context\": context, \"query\": user_question})\n","\n","    return answer\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":null,"statement_id":null,"state":"cancelled","livy_statement_state":null,"queued_time":"2023-10-27T10:53:54.99335Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":"2023-10-27T10:54:37.3140519Z","spark_jobs":null,"parent_msg_id":"3e90cc97-83e5-40eb-a59a-73ef530a6307"},"text/plain":"StatementMeta(, , , Cancelled, )"},"metadata":{}}],"execution_count":null,"metadata":{},"id":"8e4f01df-a8ff-4da9-b183-26b7d16535e5"},{"cell_type":"code","source":["get_answer(\"From the analysis what themes were observed?\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":null,"statement_id":null,"state":"cancelled","livy_statement_state":null,"queued_time":"2023-10-27T10:53:54.9939809Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":"2023-10-27T10:54:37.3141998Z","spark_jobs":null,"parent_msg_id":"4999ef91-806b-4fd3-8ceb-9c94f6eb4cca"},"text/plain":"StatementMeta(, , , Cancelled, )"},"metadata":{}}],"execution_count":null,"metadata":{},"id":"fc36cf78-9324-4a55-8274-cbab6b710ee4"},{"cell_type":"code","source":["chiller_items = get_answer('What percentage of total sales value did chiller items make? Number only')\n","dried_items = get_answer('What percentage of total sales value did other items make? Number only')\n","\n","data = [('chiller_items', int(chiller_items)), ('dried_items', int(dried_items))]\n","ts_value = spark.createDataFrame(data, ['ItemType', 'SalePercent'])\n","\n","ts_value.write.mode('overwrite').format('delta').save('Tables/' + 'ts_value')"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":null,"statement_id":null,"state":"cancelled","livy_statement_state":null,"queued_time":"2023-10-27T10:53:54.9946188Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":"2023-10-27T10:54:37.3143997Z","spark_jobs":null,"parent_msg_id":"e9f6d800-c12a-4400-9ab9-1a86c5c04216"},"text/plain":"StatementMeta(, , , Cancelled, )"},"metadata":{}}],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"3e846fa4-2368-42ca-9ec7-1b90093fbfbe"}],"metadata":{"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"host":{"synapse_widget":{"token":"ee07ea8a-35c9-4743-9d59-4e60f2447c7b","state":{}}},"language":"python","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"kernel_info":{"name":"synapse_pyspark"},"nteract":{"version":"nteract-front-end@1.0.0"},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"enableDebugMode":false,"conf":{}}},"notebook_environment":{},"synapse_widget":{"version":"0.1","state":{}},"trident":{"lakehouse":{"known_lakehouses":[{"id":"c7b2fef9-60ca-4a33-a662-941b83feaa93"}],"default_lakehouse":"c7b2fef9-60ca-4a33-a662-941b83feaa93","default_lakehouse_name":"wwilakehouse","default_lakehouse_workspace_id":"3ef9c051-6166-444f-8b3b-23112b4e0216"}}},"nbformat":4,"nbformat_minor":5}